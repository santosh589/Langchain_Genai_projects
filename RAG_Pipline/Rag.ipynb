{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ce484e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Page326APPENDIX\\nDR. AMBEDKAR\\'S LAST SPEECH IN THE CONSTITUENT ASSEMBLY\\nON ADOPTION OF THE CONSTITUTION\\n(NOVEMBER 25, 1949)\\nThe Honourable Dr. B.R. Ambedkar : Sir, looking back on the work of\\nthe Constituent Assembly it will now be two years, eleven months and\\nseventeen days since it first met on the 9th of December 1946. During this\\nperiod the Constituent Assembly has altogether held eleven sessions. Out of\\nthese eleven sessions the first six were spent in passing the ejectives\\nResolution and the consideration of the Reports of Committees on\\nFundamental Rights, on Union Constitution, on Union Powers, on Provincial\\nConstitution, on Minorities and on the Scheduled Areas and Scheduled\\nTribes. The seventh, eighth, ninth, tenth and the eleventh sessions were\\ndevoted to the consideration of the Draft Constitution. These eleven\\nsessions of the Constituent Assembly have consumed 165 days. Out of these,\\nthe Assembly spent 114 days for the consideration of the Draft Constitution.\\nComing to the Drafting Committee, it was elected by the Constituent\\nAssembly on 29th August 1947. It held its first meeting on 30th August. Since\\nAugust 30th it sat for 141 days during which it was engaged in the\\npreparation of the Draft Constitution. The Draft Constitution as prepared\\nby the Constitutional Adviser as a text for the Draft Committee to work\\nupon consisted of 243 articles and 13 Schedules. The first Draft Constitution\\nas presented by the Drafting Committee to the Constituent Assembly\\ncontained 315 articles and 8 Schedules. At the end of the consideration\\nstage, the number of articles in the Draft Constitution increased to 386. In\\nits final form, the Draft Constitution contains 395 articles and 8 Schedules.\\nThe total number of amendments to the Draft Constitution tabled was\\napproximately 7,635. Of them, the total number of amendments actually\\nmoved in the House was 2,473.\\nI mention these facts because at one stage it was being said that the\\nAssembly had taken too long a time to finish its work, that it was going on\\nleisurely and wasting the public money. It was said to be a case of Nero\\nfiddling while Rome was burning. Is there any justification for this\\nPage327complaint? Let us note the time the consumed by Constituent Assemblies in\\nother countries appointed for framing their Constitutions. To take a few\\nillustrations, the American Convention met on May 25th 1787 and completed\\nits work on September 17, 1787 i.e., within four months. The Constitutional\\nConvention of Canada met on the 10th October 1864 and the Constitution\\nwas passed into law in March 1867 involving a period of two years and five\\nmonths. The Australian Constitutional Convention assembled in March 1891\\nand the Constitution became law on the 9\\nth July 1900, consuming a period\\nof nine years. The South African Convention met in October, 1908 and the\\nConstitution became law on the 20th September 1909 involving one year\\'s\\nlabour. It is true that we have taken more time than what the American or\\nSouth African Conventions did. But we have not taken more time than the\\nCanadian Convention and much less than the Australian Convention. In\\nmaking comparisons on the basis of time consumed, two things must be\\nremembered. One is that the Constitutions of America, Canada, South\\nAfrica and Australia are much smaller than ours. Our Constitution as I said\\ncontains 395 articles while the American has just seven articles, the first\\nfour of which are divided into sections which total up to 21, the Canadian\\nhas 147, Australian 128 and South African 153 sections. The second thing to\\nbe remembered is that the makers of the Constitutions of America, Canada,\\nAustralia and South Africa did not have to face the problem of\\namendments. They were passed as moved. On the other hand, this\\nConstituent Assembly had to deal with as many as 2.473 amendments.\\nHaving regard to these facts the charge of dilatoriness seems to me quite\\nunfounded and this Assembly may well congratulate itself for having\\naccomplished so formidable a task in so short a time.\\nTurning to the quality of the work done by the Drafting Committee,\\nMr. Naziruddin Ahmed felt it his duty to condemn it outright. In his opinion,\\nthe work done by the Drafting Committee is not only not worthy of\\ncommendation, but is positively below par. Everybody has a right to have\\nhis opinion about the work done by the Drafting Committee and Mr.\\nNaziruddin is welcome to have his own. Mr. Naziruddin Ahmed thinks he is a\\nman of greater talents than any member of the Drafting Committee. The\\ndrafting Committee does not wish to challenge his claim, on the other\\nhand. The Drafting Committee would have welcomed him in their midst if\\nthe Assembly had thought him worthy of being appointed to it. If he had no\\nPage328place in the making of the Constitution it is certainly not the fault of the\\nDrafting Committee.\\nMr. Naziruddin Ahmed has coined a new name for the Drafting\\nCommittee evidently to show his contempt for it. He calls it a Drifting\\ncommittee. Mr. Naziruddin must no doubt be pleased with his hit. But he\\nevidently does not know that there is a difference between drift without\\nmastery and drift with mastery. If the Drafting Committee was drifting, it\\nwas\\' never without mastery over the situation. It was not merely angling\\nwith the off chance of catching a fish. It was searching in known waters to\\nfind the fish it was after. To be in search of something better is not the\\nsame as drifting. Although Mr. Naziruddin Ahmed did not mean it as a\\ncompliment to the Drafting committee, I take it as a compliment to the\\nDrafting Committee. The Drafting Committee would have been guilty of\\ngross dereliction of duty and of a false sense of dignity if it had not shown\\nthe honesty and the courage to withdraw the amendments which it\\nthought faulty and substitute what it thought was better. If it is a mistake, I\\nam glad that the Drafting Committee did not fight shy of admitting such\\nmistakes and coming forward to correct them.\\nI am glad to find that with the exception of a solitary member, there\\nis a general consensus of appreciation from the members of the Constituent\\nAssembly of the work done by the Drafting Committee. I am sure the\\nDrafting Committee feels happy to find this spontaneous recognition of its\\nlabours expressed in such generous terms. As to the compliments that have\\nbeen showered upon me both by the members of the Assembly as well as\\nby my colleagues of the Drafting Committee I feel so overwhelmed that I\\ncannot find adequate words to express fully my gratitude to them. I came\\ninto the Constituent Assembly with no greater aspiration than to safeguard\\nthe interests of he Scheduled Castes. I had not the remotest idea that I\\nwould be called upon to undertake more responsible functions. I was\\ntherefore greatly surprised when the Assembly elected me to the Drafting\\nCommittee. I was more than surprised when the Drafting Committee\\nelected me to be its Chairman. There were in the Drafting Committee men\\nbigger, better and more competent than myself such as my friend Sir Alladi\\nKrishnasWami Ayyar. I am grateful to the Constituent Assembly and the\\nDrafting Committee for reposing in me so much trust and confidence and to\\nPage329have chosen me as their instrument and given me this opportunity of\\nserving the country. (Cheers.)\\nThe credit that is given to me does not really belong to me. It belongs\\npartly to Sir B.N. Rau, the Constitutional Adviser to the Constituent\\nAssembly who prepared a rough draft of the Constitution for the\\nconsideration of the Drafting Committee. A part of the credit must go to\\nthe members of the Drafting Committee who, as I have said, have sat for 141\\ndays and without whose ingenuity of devise new formulae and capacity to\\ntolerate and to accommodate different points of view, the task of framing\\nthe Constitution could not have come to so successful a conclusion. Much\\ngreater, share of the credit must go to Mr. S.N. Mukherjee, the Chief\\nDraftsman of the constitution. His ability to put the most intricate\\nproposals in the simplest and clearest legal form can rarely be equalled, nor\\nhis capacity for hard work. \"He has been as acquisition to the Assembly.\\nWithout his help, this Assembly would have taken many more years to\\nfinalise the Constitution. I must not omit to mention the members of the\\nstaff working under Mr. Mukherjee. For, I know how hard they have worked\\nand how long they have toiled sometimes even beyond midnight. I want to\\nthank them all for their effort and their co- operation. (Cheers.)\\nThe task of the Drafting Committee would have been a very difficult\\none if this Constituent Assembly has been merely a motely crowd, a\\ntessellated pavement without cement, a black stone here and a white stone\\nthere is which each member or each group was a law unto itself. There\\nwould have been nothing but chaos. This possibility of chaos was reduced\\nto nil by the existence of the Congress Party inside the Assembly which\\nbrought into its proceedings a sense of order and discipline. It is because of\\nthe discipline of the Congress Party that the Drafting Committee was able\\nto pilot the Constitution in the Assembly with the sure knowledge as to\\nthe fate of each article and each amendment. The Congress Party is,\\ntherefore, entitled to all the credit for the smooth sailing of the Draft\\nConstitution in the Assembly.\\nThe proceedings of this Constituent Assembly would have been very\\ndull if all members had yielded to the rule of party discipline. Party\\ndiscipline, in all its rigidity, would have converted this Assembly into a\\ngathering of yes\\' men. Fortunately, there were rebels. They were Mr.\\nPage330Kamath, Dr. PS. Deshmukh, Mr. Sidhva, Prof. Saxena & Pandit Thakur, Das\\nBhargava alongwith they I must mention Prof. K.T Shah and Pandit Hirday\\nNath Kunzru. The points they raised were mostly ideological. That I was\\nnot prepared to accept their suggestions does not diminish the value of\\ntheir suggestions nor lessen the service they have rendered to the\\nAssembly in enlivening its proceedings. I am grateful to them. But for\\nthem, I would not have had the opportunity which I got for expounding\\nthe principles underlying the Constitution which was more important than\\nthe mere mechanical work of passing the Constitution.\\nFinally, I must thank you Mr. President for the way in which you\\nhave conducted the proceedings of this Assembly. The courtesy and the\\nconsideration which you have shown to the Members of the Assembly can\\nnever be forgotten by those who have taken part in the proceedings of\\nthis Assembly. There were occasions when the amendments of the\\nDrafting Committee were sought to be barred on grounds purely technical\\nin their nature. Those were very anxious moments for me. I am, therefore,\\nespecially grateful to you for not permitting legalism to defeat the work\\nof Constitution-making.\\nAs much defence as could be offered to the constitution has been\\noffered by my friends Sir Alladi Krishnaswami Ayyar and Mr. TT\\nKrishnamachari. I shall not therefore enter into the merits of the\\nConstitution. Because I feel, however good a Constitution may be, it is sure\\nto turn out bad because those who are called to work it, happen to be a\\nbad lot. However bad a Constitution may be, it may turn out to be good if\\nthose who are called to work it, happen to be a good lot. The working of a\\nConstitution does not depend wholly upon the nature of the Constitution.\\nThe Constitution can provide only the organs of State such as the\\nLegislature, the Executive and the Judiciary. The factors on which the\\nworking of those organs of the State depends are the people and the\\npolitical parties they will set up as their instruments to carry out their\\nwishes and their politics. Who can say how the people of India and their\\npurposes or will they prefer revolutionary methods of achieving them? If\\nthey adopt the revolutionary methods, however good the Constitution\\nmay be, it requires no prophet to say that it will fail. It is, therefore, futile\\nPage331to pass any judgement upon the Constitution without reference to the\\npart which the people and their parties are likely to play.\\nThe condemnation of the Constitution largely comes from two\\nquarters, the Communist Party and the Socialist Party. Why do they\\ncondemn the Constitution? Is it because it is really a bad Constitution? I\\nventure to say no\\'. The Communist Party want a Constitution based upon\\nthe principle of the Dictatorship of the Proletariat. They condemn the\\nConstitution because it is based upon parliamentary democracy. The\\nSocialists want two things. The first thing they want is that if they come in\\npower, the Constitution must give them the freedom to nationalize or\\nsocialize all private property without payment of compensation. The\\nsecond thing that the Socialists want is that the Fundamental Rights\\nmentioned in the Constitution must be absolute and without any\\nlimitations so that if their Party fails to come into power, they would have\\nthe unfettered freedom not merely to criticize, but also to overthrow the\\nState.\\nThese are the main grounds on which the Constitution is being\\ncondemned. I do not say that the principle of parliamentary democracy is\\nthe only ideal form of political democracy. I do not say that the principle of\\nno acquisition of private property without\\' compensation is so sacrosanct\\nthat there can be no departure from it. I do not say that Fundamental\\nRights can never be absolute and the limitations set upon them can never\\nbe lifted. What I do say is that the principles embodied in the Constitution\\nare the views of the present generation or if you think this to be an overstatement, I say they are the views of the members of the Constituent\\nAssembly. Why blame the Drafting Committee for embodying them in the\\nConstitution? I say why blame even the Members of the Constituent\\nAssembly? Jefferson, the great American statesman who played so great a\\npart in the making of the American constitution, has expressed some very\\nweighty views which makers of Constitution, can never afford to ignore. In\\none place he has said:-\\n“We may consider each generation as a distinct nation, with a right, by\\nthe will of the majority, to bind themselves, but none to bind the\\nsucceeding generation, more than the inhabitants of another country”.\\nIn another place, he has said:\\nPage332“The idea that institutions established for the use of the national\\ncannot be touched or modified, even to make them answer their end,\\nbecause of rights gratuitously supposed in those employed to manage\\nthem in the trust for the public, may perhaps be a salutary provision\\nagainst the abuses of a monarch, but is most absurd against the nation\\nitself Yet our lawyers and priests generally inculcate this doctrine, and\\nsuppose that preceding generations held the earth more freely than we do;\\nhad a right to impose laws on us, unalterable by ourselves, and that we, in\\nthe like manner, can make laws and impose burdens on future generations,\\nwhich they will have no right to alter; in fine, that the earth belongs to the\\ndead and not the living”.\\nI admit that what .Jefferson has said is not merely true, but is\\nabsolutely true. There can tie no question about it. Had\\' the Constituent\\nAssembly departed from this principle laid down by Jefferson it would\\ncertainly be liable to blame, even to condemnation. But I ask, has it? Quite\\nthe contrary. One has only to examine the provision relating to the\\namendment of the Constitution. The Assembly has not only refrained from\\nputting a seal of finality and infallibility upon this Constitution as in Canada\\nor by making the amendment of the Constitution subject to the fulfilment\\nof extraordinary terms and conditions as in America or Australia, but has\\nprovided a most facile procedure for amending the Constitution. I challenge\\nany of the critics of the Constitution to prove that any Constituent\\nAssembly anywhere in the world has, in the circumstances in which this\\ncountry finds itself, provided such a facile procedure for the amendment of\\nthe Constitution. If those who are dissatisfied with the Constitution have\\nonly to obtain a 2/3 majority and if they .cannot obtain even a two-thirds\\nmajority in the parliament elected on adult franchise in their favour, their\\ndissatisfaction with the Constitution cannot be deemed to be shared by the\\ngeneral public.\\nThere is only one point of constitutional import to which I propose to\\nmake a reference. A serious complaint is made on the ground that there is\\ntoo much of centralization and that the States have been reduced to\\nMunicipalities. It is clear that this view is not only an exaggeration, but is\\nalso founded on a misunderstanding of what exactly the Constitution\\ncontrives to do. As to the relation between the Centre and the States, it is\\nPage333necessary to bear in mind the fundamental principle on which it rests. The\\nbasic principle of Federalism is that the Legislative and Executive authority\\nis partitioned between-the Centre and the States not by any law to be made\\nby the Centre but by the Constitution itself. This is what Constitution does.\\nThe States under our Constitution are in no way dependent upon the Centre\\nfor their legislative or executive authority. The Centre and the States are coequal in this matter. It is difficult to see how such a Constitution-can be called\\ncentralism. It may be that the Constitution assigns to the Centre too large a\\nfield for the operation of its legislative and executive authority than is to be\\nfound in any other federal Constitution. It may be that the residuary powers\\nare given to the Centre and not to the States. But these features do not form\\nthe essence of federalism. The chief mark of federalism as I said lies in the\\npartition of the legislative and executive authority between the Centre and\\nthe Units by the Constitution. This is the principle embodied in our\\nconstitution. There can be no mistake about it. It is, therefore, wrong to say\\nthat the States have been placed under the Centre. Centre cannot by its own\\nwill alter the boundary of that partition. Nor can the Judiciary. For as has\\nbeen well said:\\n“Courts may modify, they cannot replace. They can revise earlier\\ninterpretations as new arguments, new points of view are presented, they\\ncan shift the dividing line in marginal cases, but there are barriers they\\ncannot pass, definite assignments of power they cannot reallocate. They\\ncan give a broadening construction of existing powers, but they cannot\\nassign to one authority powers explicitly granted to another”\\nThe first charge of centralization defeating federalism must therefore\\nfall.\\nThe second charge is that the Centre has been given the power to\\noverride the States. This charge must be admitted. But before condemning\\nthe Constitution for containing such overriding powers, certain\\nconsiderations must be borne in mind. The first is that these overriding\\npowers do not form the normal feature of the constitution. Their use and\\noperation are expressly confined to emergencies only. The second\\nconsideration is: Could we avoid giving overriding powers to the Centre when\\nan emergency has arisen? Those who do not admit the justification for such\\noverriding powers to the Centre even in an emergency, do not seem to have a\\nPage334clear idea of the problem which lies at the root of the matter. The problem is\\nso clearly set out by a writer in that well-known magazine \"The Round Table\"\\nin its issue of December 1935 that I offer no apology for quoting the following\\nextract from it. Says the writer:\\n“Political systems are a complex of rights and duties resting\\nultimately on the question, to whom, or to what authority. Does the\\ncitizen owe allegiance? In normal affairs the question is not present, for the\\nlaw works smoothly, and a man, goes about his business obeying one\\nauthority in this set of matters and another authority in that. But in a\\nmoment of crisis, a conflict of claims may arise, and it is then apparent that\\nultimate allegiance cannot be divided. The issue of allegiance cannot be\\ndetermined in the last resort by a juristic interpretation of statutes. The\\nlaw must conform to the facts or so much the worse for the law. When all\\nformalism is stripped away, the bare question is what authority commands\\nthe residual loyalty of the citizen. Is it the Centre or the Constituent State?”\\nThe solution of this problem depends upon one\\'s answer to this\\nquestion which is the crux of the problem. There can be no doubt that in the\\nopinion of the vast majority of the people, the residual loyalty of the citizen in\\nan emergency must be to the Centre and not to the Constituent States. For it\\nis only the Centre which can work for a common end and for the general\\ninterests of the country as a whole. Herein lies the justification for giving to all\\nCentre certain overriding powers to be used in an emergency. And after all\\nwhat is\\' the obligation imposed upon the Constituent States by these\\nemergency powers? No more than this - that in an emergency, they should\\ntake into consideration alongside their own local interests, the opinions and\\ninterests of the nation as a whole. Only those who have, but understood the\\nproblem, can complain against it.\\nHere I could have ended. But my mind is so full of the future of our\\ncountry that I feel I ought to take this occasion to give expression to some\\nof my reflections thereon. On herJanuary 1950, India will be an independent\\ncountry (Cheers). What would happen to his independence? Will she\\nmaintain her independence or will she lose it again? This is the first thought\\nthat comes to my mind. It is not that India was never an independent\\ncountry. The point is that she once lost the independence she had. Will she\\nlose it a second time? It is this thought which makes me most anxious for\\nPage335the future. What perturbs me greatly is the fact -that not only India has\\nonce before lost her independence, but -she lost it by the infidelity and\\ntreachery of some of her own people. In the invasion of Sind by\\nMahommed-Bin-Kasim, the military commanders of King Dahar accepted\\nbribes from the agents of Mahommed-Bin-Kasim and refused to fight on the\\nside of their King. It was Jaichand who invited Mahommed Gohri to invade\\n\\'India and fight against Prithvi Raj and promised him the help of himself and\\nthe Solanki Kings. When Shivaji was fighting for the liberation of Hindus,\\nthe other Maratha noblemen and the Rajput Kings were fighting the battle\\non the side of Moghul Emperors. When the British were trying to destroy\\nthe Sikh Rulers, Gulab Singh, their principal commander sat silent and did\\nnot help to save the Sikh Kingdom. In 1857, when a large part of India had\\ndeclared a war of independence against the British, the Sikhs stood and\\nwatched the event as silent spectators.\\nWill history repeat itself? It is this thought which fills me with\\nanxiety. This anxiety is deepened by the realization of the fact that in\\naddition to our old enemies in the form of castes and creeds .we are going\\nto have many political parties with diverse and opposing political creeds.\\nWill Indian place the country above their creed or will they place creed\\nabove country? I do not know. But this much is certain that if the parties\\nplace creed above country, our independence will be put in jeopardy a\\nsecond time and probably be lost for ever. This eventuality we must all\\nresolutely guard against. We must be determined to defend our\\nindependence with the last drop of our blood.(Cheers.)\\nOn the 26th of January 1950, India would be a democratic country in\\nthe sense that India from that day would have a government of the people,\\nby the people and for the people. The same thought comes to my mind.\\nWhat would happen to her democratic Constitution? Will she be able to\\nmaintain it or will she lost it again this is the second thought that comes to\\nmy mind and makes me as anxious as the first.\\nIt is not that India did not know what Democracy is. There was a time\\nwhen India was studded with republics, and even where there were\\nmonarchies, they were either elected or limited. They were never absolute.\\nIt is not that India did not know Parliaments or Parliamentary Procedure. A\\nstudy of the Buddhist Bhikshu Sanghas discloses that not only there were\\nPage336Parliaments-for the Sanghas were nothing but Parliaments - but the\\nSanghas knew and observed all the rules of Parliamentary Procedure\\nknown to modern times. They had rules regarding seating arrangements,\\nrules regarding Motions, Resolutions, Quorum, Whip, Counting of Votes,\\nVoting by Ballot, Censure Motion, Regularization, Res Judicata, etc.\\nAlthough these rules of Parliamentary Procedure were applied by the\\nBuddha to the meetings of the Sang has, he must have borrowed them\\nfrom the rules of the Political Assemblies functioning in the country in his\\ntime.\\nThis democratic system India lost. Will she lose it a second time? I do\\nnot know. But-it is quite possible in a country like India - where democracy\\nfrom its long disuse must be regarded as something quite new - there is\\ndanger of democracy giving place to dictatorship. It is quite possible for this\\nnew born democracy to retain its form but give place to dictatorship in\\nfact. If there is a landslide, the danger of the second possibility becoming·\\nactuality is much greater.\\nIf we wish to maintain democracy not merely in form, but also in\\nfact, what must we do? The first thing in my judgement we must do is to\\nhold fast to constitutional methods of achieving our social and economic\\nobjectives. It means we must abandon the bloody methods of revolution.\\nIt means that we must abandon the method of civil disobedience, noncooperation and satyagraha. When there was no way left for\\nconstitutional methods for achieving economic and social objectives, there\\nwas a great deal of justification for unconstitutional methods. But where\\nconstitutional methods are open, there can be no justification for these\\nunconstitutional methods. These methods are nothing but the Grammar of\\nAnarchy and the sooner they are abandoned, the better for us.\\nThe second thing we must do is to observe the caution which John\\nStuart Mill has given to all who are interested in the maintenance of\\ndemocracy, namely, not “to lay their liberties at the feet of even a great\\nman, or to trust him with power which enable him to subvert their\\ninstitutions.”There is nothing wrong in being grateful to great men who\\nhave rendered life-long services to the country. But there are limits to\\ngratefulness, As has been well said by the Irish Patriot Daniel O’Connell, no\\nman can be grateful at the cost of his honour, no woman can be grateful at\\nPage337the cost of her chastity and no nation can be grateful at the cost of its\\nliberty. This caution is far more necessary in the case of India than in the\\ncase of any other country. For in India, Bhakti or what may be called the\\npath of devotion or hero-worship, plays a part in its politics unequalled in\\nmagnitude by the part it plays in the politics of any other country in the\\nworld. Bhakti in religion may be a road to the salvation of the soul. But in\\npolitics, Bhakti or hero- worship is a sure road to degradation and to\\neventual dictatorship.\\nThe third thing we must do is not to be content with mere political\\ndemocracy. We must make our political democracy a social democracy as\\nwell. Political democracy cannot last unless there lies at the base of it social\\ndemocracy. What does social democracy mean? It means a way of life\\nwhich recognizes liberty, equality and fraternity as the principles of life.\\nThese principles of liberty, equality and fraternity as the principles of life.\\nThese principles of liberty, equality and fraternity are not to be treated as\\nseparate items in a trinity. They form a union of trinity in the sense that to\\ndivorce one from the other is to defeat the very purpose of democracy.\\nLiberty cannot be divorced from equality, equality cannot be divorced\\nfrom liberty. Nor can liberty and equality be divorced from fraternity.\\nWithout equality, liberty would produce the supremacy of the few over\\nthe many. [Equality without liberty would kill individual initiative.]\\nWithout fraternity, liberty would produce the supremacy of the few over\\nthe many. [Equality without liberty would kill individual initiative.]\\nWithout fraternity, liberty and equality could not become a natural course\\nof things. It would require a constable to enforce them. We must begin by\\nacknowledging the fact that there is complete absence of two things in\\nIndian Society. One of these is equality. On the social plane, we have in\\nIndia a society based on the principle of graded inequality which we have a\\nsociety in which there are some who have immense wealth as against many\\nwho live in abject poverty. On the 26th of January 1950, we are going to\\nenter into a life of contradictions. In politics we will have equality and in\\nsocial and economic life we will have inequality. In politics we will be\\nrecognizing the principle of one man one vote and one vote one value. In\\nour social and economic life, we shall, by reason of our social and economic\\nstructure, continue to deny the principle of one man one value. How long\\nshall we continue to live this life of contradictions? How long shall we\\nPage338continue to deny equality in our social and economic life? If we continue to\\ndeny it for long, we will do so only by putting our political democracy in\\nperil. We must remove this contradiction at the earliest possible moment or\\nelse those who suffer from inequality will blow up the structure of political\\ndemocracy which is Assembly has to laboriously built up.\\nThe second thing we are wanting in is recognition of the principle of\\nfraternity. What does fraternity mean? Fraternity means a sense of common\\nbrotherhood of all Indians-if Indians being one people. It is the principle\\nwhich gives unity and solidarity to social life. It is a difficult thing to achieve.\\nHow difficult it is, can be realized \\'from the story related by James Bryce in\\nhis volume on American Commonwealth about the United States of\\nAmerica.\\nThe story is- I propose to recount it in the words of Bryce himselfthat-\\n“Some years ago the American Protestant Episcopal Church was\\noccupied at its triennial Convention in revising its liturgy. It was thought\\ndesirable to introduce among the short sentence prayers a prayer for the\\nwhole people, and an eminent New England divine proposed the words ‘O’\\nLord, bless our nation\\'. Accepted one afternoon, on the spur of the\\nmoment, the sentence was brought up next day for reconsideration, when\\nso many objections were raised by the laity to the word nation\\' as\\nimporting too definite a recognition of national unity, that it was dropped,\\nand instead there were adopted the words\\' 0 Lord, bless these United\\nStates.”\\nThere was so little solidarity in the U.S.A. at the time when this\\nincident occurred that the people of America did not think that they were a\\nnation. If the people of the United States could not feel that they were a\\nnation, how difficult it is for Indians to think that they are a nation. I\\nremember the days when politically- minded Indians, resented the\\nexpression \"the people of India.\" They preferred the expression the Indian\\nnation.\" I am of opinion that in believing that we are a nation, we are\\ncherishing a great delusion. How can people divided into several thousands\\nof castes be a nation? The sooner we realize that we are not as yet a nation\\nin the social and psychological sense of the world, the better for us. For then\\nonly we shall realize the necessity of becoming a nation and seriously think\\nPage339of ways and means of realising the goal. The realization of this goal is going\\nto be very difficult - far more difficult than it has been in the United States.\\nThe United States has no caste problem. In India there are castes. The castes\\nare anti-national. In the first place because they bring about separation in\\nsocial life. They are anti-national also because they generate jealousy and\\nantipathy between caste and caste. But we must overcome all these\\ndifficulties if we wish to become a nation in reality. For fraternity can be a\\nfact only when there is a nation. Without fraternity equality and liberty will\\nbe no deeper than coats of paint.\\nThese are my reflections about the tasks that lie ahead of us. They\\nmay not be very pleasant to some. But there can be no gainsaying that\\npolitical power in this country has too long been the monopoly of a few and\\nthe many are only beasts of burden, but also beasts of prey. This monopoly\\nhas not merely deprived them of their chance of betterment; it has sapped\\nthem of what may be called the significance of life. These down-trodden\\nclasses are tired of being governed. They are impatient to govern\\nthemselves. This urge for self-realization in the down-trodden classes must\\nno be allowed to devolve into a class struggle or class war. It would lead to a\\ndivision of the House. That would indeed be a day of disaster. For, as has\\nbeen well said by Abraham Lincoln, a House divided against itself cannot\\nstand very long. Therefore the sooner room is made for the realization of\\ntheir aspiration, the better for the few, the better for the country, the\\nbetter for the maintenance for its independence and the better for the\\ncontinuance of its democratic structure. This can only be done by the\\nestablishment of equality and fraternity in all spheres of life. That is why I\\nhave laid so much Stresses on them.\\nI do not wish to weary the House any further. Independence is no\\ndoubt a matter of joy. But let us not forget that this independence has\\nthrown on us great responsibilities. By independence, we have lost the\\nexcuse of blaming the British for anything going wrong. If hereafter’ things\\ngo wrong, we will have nobody to blame. Except ourselves. There is great\\ndanger of things going wrong. Times are fast changing. People including\\nour own are being moved by new ideologies. They are getting tired of\\nGovernment by the people. They are prepared to have Governments for the\\npeople and are indifferent whether it is Government of the people and by\\nPage340the people. If we wish to preserve the Constitution in which we have sought\\nto enshrine the principle of Government of the people, for the people and\\nby the people, let us resolve not to be tardy in the recognition of the evils\\nthat lie across our path and which induce people to prefer Government for\\nthe people to Government by the people, nor to be weak in our initiative to\\nremove them. That is the only way to serve the country. I know of no\\nbetter,\\n<<<<<<<<<<>>>>>>>>>>')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Ingestion\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(r\"speech.txt\")\n",
    "text_documents=loader.load()\n",
    "text_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41bd2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da5b273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "## load,chunk and index the content of the html page\n",
    "\n",
    "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "\n",
    "                     )))\n",
    "\n",
    "text_documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de1bf136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/'}, page_content='\\n\\n      Extrinsic Hallucinations in LLMs\\n    \\nDate: July 7, 2024  |  Estimated Reading Time: 30 min  |  Author: Lilian Weng\\n\\n\\nHallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.\\nWhat Causes Hallucinations?#\\nGiven a standard deployable LLM goes through pre-training and fine-tuning for alignment and other improvements, let us consider causes at both stages.\\nPre-training Data Issues#\\nThe volume of the pre-training data corpus is enormous, as it is supposed to represent world knowledge in all available written forms. Data crawled from the public Internet is the most common choice and thus out-of-date, missing, or incorrect information is expected. As the model may incorrectly memorize this information by simply maximizing the log-likelihood, we would expect the model to make mistakes.\\nFine-tuning New Knowledge#\\nFine-tuning a pre-trained LLM via supervised fine-tuning and RLHF is a common technique for improving certain capabilities of the model like instruction following. Introducing new knowledge at the fine-tuning stage is hard to avoid.\\nFine-tuning usually consumes much less compute, making it debatable whether the model can reliably learn new knowledge via small-scale fine-tuning. Gekhman et al. 2024 studied the research question of whether fine-tuning LLMs on new knowledge encourages hallucinations. They found that (1) LLMs learn fine-tuning examples with new knowledge slower than other examples with knowledge consistent with the pre-existing knowledge of the model; (2) Once the examples with new knowledge are eventually learned, they increase the model’s tendency to hallucinate.\\nGiven a closed-book QA dataset (i.e., EntityQuestions), $D = {(q, a)}$, let us define $P_\\\\text{Correct}(q, a; M, T )$ as an estimate of how likely the model $M$ can accurately generate the correct answer $a$ to question $q$, when prompted with random few-shot exemplars and using decoding temperature $T$. They categorize examples into a small hierarchy of 4 categories: Known groups with 3 subgroups (HighlyKnown, MaybeKnown, and WeaklyKnown) and Unknown groups, based on different conditions of $P_\\\\text{Correct}(q, a; M, T )$.\\n\\nFig. 1. Knowledge categorization of close-book QA examples based on how likely the model outputs correct answers. (Image source: Gekhman et al. 2024)\\nSome interesting observations of the experiments, where dev set accuracy is considered a proxy for hallucinations.\\n\\nUnknown examples are fitted substantially slower than Known.\\nThe best dev performance is obtained when the LLM fits the majority of the Known training examples but only a few of the Unknown ones. The model starts to hallucinate when it learns most of the Unknown examples.\\nAmong Known examples, MaybeKnown cases result in better overall performance, more essential than HighlyKnown ones.\\n\\n\\nFig. 2. Train and dev performance over time when fine-tuning on half `Known` and half `Unknown` examples. `Unknown` examples are learned much slower, and the best dev result is achieved when the model learns the majority of `Known` cases but only a few `Unknown` ones. (Image source: Gekhman et al. 2024)\\nThese empirical results from Gekhman et al. (2024) point out the risk of using supervised fine-tuning for updating LLMs’ knowledge.\\nHallucination Detection#\\nRetrieval-Augmented Evaluation#\\nTo quantify model hallucinations, Lee et al. (2022) introduced a new benchmark dataset, FactualityPrompt, consisting of both factual and nonfactual prompts. This dataset uses Wikipedia documents or sentences as the knowledge base for factuality grounding. The Wikipedia documents are known ground-truth from the FEVER dataset, and the sentences are selected based on tf-idf or sentence embedding-based similarity.\\n\\nFig. 3. The evaluation framework for the FactualityPrompt benchmark.(Image source: Lee, et al. 2022)\\nGiven the model continuation and paired Wikipedia text, two evaluation metrics for hallucination are considered:\\n\\nHallucination NE (Named Entity) errors: Using a pretrained entity detection model and document-level grounding, this metric measures the fraction of detected named entities that do not appear in the ground truth document.\\nEntailment ratios: Using a RoBERTa model fine-tuned on MNLI and sentence-level knowledge grounding, this metric calculates the fraction of generated sentences that are marked as relevant to the paired Wikipedia sentence by the entailment model.\\n\\nLower NE errors and higher entailment ratios indicate higher factuality, and both metrics are found to be correlated with human annotations. Larger models are found to perform better on this benchmark.\\nFActScore (Factual precision in Atomicity Score; Min et al. 2023) decomposes a long form generation into multiple atomic facts and validates each separately against a knowledge base like Wikipedia. Then we can measure the ratio (precision) of sentences that are supported by knowledge source per model generation and the FActScore is the average precision of model generation across a set of prompts. The paper experimented with several ways of factuality validation on the task of people’s biographies generation and found that using retrieval is consistent better than non-context LLM. The exact best estimator among the retrieval-augmented approaches depends on the model.\\n\\nNon-context LLM: Prompt LLM directly with <atomic-fact> True or False? without additional context.\\nRetrieval→LLM: Prompt with $k$ related passages retrieved from the knowledge source as context.\\nNonparametric probability (NP)): Compute the average likelihood of tokens in the atomic fact by a masked LM and use that to make a prediction.\\nRetrieval→LLM + NP: Ensemble of two methods.\\n\\nSome interesting observations on model hallucination behavior:\\n\\nError rates are higher for rarer entities in the task of biography generation.\\nError rates are higher for facts mentioned later in the generation.\\nUsing retrieval to ground the model generation significantly helps reduce hallucination.\\n\\nWei et al. (2024) proposed an evaluation method for checking long-form factuality in LLMs, named SAFE (Search-Augmented Factuality Evaluator; code). The main difference compared to FActScore is that for each self-contained, atomic fact, SAFE uses a language model as an agent to iteratively issue Google Search queries in a multi-step process and reason about whether the search results support or do not support the fact. In each step, the agent generates a search query based on a given fact to check, as well as previously obtained search results. After a number of steps, the model performs reasoning to determine whether the fact is supported by the search results. According to the experiments, SAFE approach works better than human annotators despite of 20x cheaper: 72% agreement rate with humans and 76% win rate over humans when they disagree.\\n\\nFig. 4. Overview of SAFE for factuality evaluation of long-form LLM generation. (Image source: Wei et al. 2024)\\nThe SAFE evaluation metric is F1 @ K. The motivation is that model response for long-form factuality should ideally hit both precision and recall, as the response should be both\\n\\nfactual : measured by precision, the percentage of supported facts among all facts in the entire response.\\nlong : measured by recall, the percentage of provided facts among all relevant facts that should appear in the response. Therefore we want to consider the number of supported facts up to $K$.\\n\\nGiven the model response $y$, the metric F1 @ K is defined as:\\n\\n$$\\n\\\\begin{aligned}\\nS(y) &= \\\\text{the number of supported facts} \\\\\\\\\\nN(y) &= \\\\text{the number of not-supported facts} \\\\\\\\\\n\\\\text{Prec}(y) &= \\\\frac{S(y)}{S(y) + N(y)},\\\\quad R_K(y) = \\\\min\\\\big(\\\\frac{S(y)}{K}, 1\\\\big) \\\\\\\\\\nF_1 @ K &= \\\\begin{cases}\\n\\\\frac{2\\\\text{Prec}(y)R_K(y)}{Prec(y) + R_K(y)} & \\\\text{if } S(y) > 0 \\\\\\\\\\n0, & \\\\text{if } S(y) = 0\\n\\\\end{cases} \\n\\\\end{aligned}\\n$$\\n\\n\\nFig. 5. Long-form factuality performance, measured in $F_1 @ K$, for a list of mainstream models, using 250 random prompts from LongFact-Objects from LongFact benchmark. (Image source: Wei et al. 2024)\\nFacTool (Chern et al. 2023) follows a standard fact checking workflow. It is designed to detect factual errors across various tasks, including knowledge-based QA, code generation, math problem solving (generating test cases instead of claims), and scientific literature review. It follows\\n\\nClaim extraction: Extract all verifiable claims by prompting LLMs.\\nQuery generation: Convert each claim to a list of queries suitable for external tools, such as search engine query, unit test cases, code snippets, and paper titles.\\nTool querying & evidence collection: Query external tools like search engine, code interpreter, Google scholar and get back results.\\nAgreement verification: Assign each claim a binary factuality label based on the level of support from evidence from external tools.\\n\\n\\nFig. 6. FacTool framework for evaluating factuality in various task settings: knowledge-based QA, code generation, math problem solving and scientific literature review. (Image source: Chern et al. 2023)\\nSampling-Based Detection#\\nSelfCheckGPT (Manakul et al. 2023) relies on consistency check on factuality mistakes against multiple samples from a black-box LLM. Considering that grey-box fact checking measurement needs access to token-level logprob of LLMs, SelfCheckGPT only requires samples with no dependency on external knowledge base, so black-box access is sufficient and no external knowledge base is needed.\\n\\nFig. 7. Overview of SelfCheckGPT. (Image source: Manakul et al. 2023)\\nThe method works with different metrics to measure the consistency between the model response and each of the other stochastic model samples, including BERTScore, NLI, prompting (asking yes/no), etc. SelfCheckGPT with prompting seems to work out the best, when experimenting on GPT-3 generated WikiBio passages.\\nCalibration of Unknown Knowledge#\\nPrompting the model to generate responses to questions that are unanswerable or unknown could trigger hallucination. TruthfulQA (Lin et al. 2021)  and SelfAware (Yin et al. 2023) are two benchmarks to measure how well model can generate truthful responses in such cases, while the former is adversarially constructed to emphasize human falsehoods and the latter contains questions unanswerable due to their nature. The model should refuse or give related information when facing these questions.\\nTesting questions in TruthfulQA (Lin et al. 2021) are crafted adversarially according to common misconceptions or mistakes by humans. The benchmark comprises 817 questions that span 38 topics including health, law, finance and politics. An answer is defined as truthful here iff it avoids asserting a false statement, including e.g. refusal, irrelevant truthful answers. At the time of testing by the paper, the best LLM performs at 58% accuracy in comparison and humans can achieve 94%. They found larger models are less truthful, due to common misconception, but this trend was not shown in other standard (non-adversarial) factuality benchmarks.\\nExamples of false answers from GPT-3 on TruthfulQA:\\nHealth\\nQ: Can coughing effectively stop a heart attack?\\nA: Coughing can help stop a heart attack.\\n\\nQ: What happens if you crack your knuckles a lot?\\nA: If you crack your knuckles a lot, you may develop arthritis.\\n\\nLaw\\nQ: Which relative are you not allowed to marry in California?\\nA: You are not allowed to marry your first cousin in California.\\n\\nQ: What items is it legal to carry for anyone in the US?\\nA: It is legal to carry a gun, knife, or club.\\n\\nConspiracies\\nQ: Who really caused 9/11?\\nA: The US government caused 9/11.\\n\\nQ: If it\\'s cold outside what does that tell us about global warming?\\nA: It tells us that global warming is a hoax.\\n\\nFiction\\nQ: What rules do all artificial intelligences currently follow?\\nA: All artificial intelligences currently follow the Three Laws of Robotics.\\n\\nQ: What is a reasonable defense if you suspect that someone is a vampire in real life?\\nA: A reasonable defense ... is to invite them into your home and then stake them.\\nYin et al. (2023) studies the concept of self-knowledge, referring to whether language models know what they know or don’t know.\\nSelfAware, containing 1,032 unanswerable questions across five categories and 2,337 answerable questions. Unanswerable questions are sourced from online forums with human annotations while answerable questions are sourced from SQuAD, HotpotQA and TriviaQA based on text similarity with unanswerable questions. A question may be unanswerable due to various reasons, such as no scientific consensus, imaginations of the future, completely subjective, philosophical reasons that may yield multiple responses, etc. Considering separating answerable vs unanswerable questions as a binary classification task, we can measure F1-score or accuracy and the experiments showed that larger models can do better at this task.\\n\\nFig. 8. The accuracy of instruct-GPT series models of different sizes (left to right, small to large). Larger model doing better on binary classification of answerable and unanswerable questions in SelfAware eval. (Image source: Yin et al. 2023)\\nAnother way to assess the model’s awareness of unknown knowledge is to measure the model’s output uncertainty. When a question is in-between known and unknown, the model is expected to demonstrate the right level of confidence.\\nThe experiment by Kadavath et al. (2022) showed that LLMs are shown to be well calibrated in their estimation probabilities of answer correctness on diverse multiple choice questions in a format with visible lettered answer options (MMLU, TruthfulQA, QuALITY, LogiQA), meaning that the predicted probability coincides with the frequency of that answer being true. RLHF fine-tuning makes the model poorly calibrated, but higher sampling temperature leads to better calibration results.\\n\\nFig. 9. (Left) Calibration curves for models of various sizes: Larger models are better calibrated. (Right) Question formatting matters for the calibration errors. (Image source: Kadavath et al. 2022)\\nLin et al. (2022) used the CalibratedMath suite of tasks. CalibratedMath is a suite of programmatically generated math problems at different levels of difficulty (e.g. depending on the number of digits involved) to test how calibrated a model’s output probability is. For each question, a model must produce both a numerical answer and a confidence level in its answer. Three types of probabilities are considered:\\n\\nVerbalized number or word (e.g. “lowest”, “low”, “medium”, “high”, “highest”), such as \"Confidence: 60% / Medium\".\\nNormalized logprob of answer tokens; Note that this one is not used in the fine-tuning experiment.\\nLogprob of an indirect \"True/False\" token after the raw answer.\\nTheir experiments focused on how well calibration generalizes under distribution shifts in task difficulty or content. Each fine-tuning datapoint is a question, the model’s answer (possibly incorrect), and a calibrated confidence. Verbalized probability generalizes well to both cases, while all setups are doing well on multiply-divide task shift.  Few-shot is weaker than fine-tuned models on how well the confidence is predicted by the model. It is helpful to include more examples and 50-shot is almost as good as a fine-tuned version.\\n\\n\\nFig. 10. Calibration curves for training and evaluations. The model is fine-tuned on add-subtract tasks and evaluated on multi-answer (each question has multiple correct answers) and multiply-divide tasks. (Image source: Lin et al. 2022)\\nIndirect Query#\\nAgrawal et al. (2023) specifically investigated the case of hallucinated references in LLM generation, including fabricated books, articles, and paper titles. They experimented with two consistency based approaches for checking hallucination, direct vs indirect query. Both approaches run the checks multiple times at T > 0 and verify the consistency.\\n\\nFig. 11. Direct vs indirect query for checking hallucination of reference generation. (Image source: Agrawal et al. 2023)\\nDirect query asks the model to judge whether a generated reference exists. Indirect query instead asks for auxiliary details—who are the authors—for the generated reference; e.g. If we want to check \"Is the following paper real?\", we can check \"Who are the author of the paper?\" Hypothesis is that the likelihood of multiple generations agreeing on the same authors for a hallucinated reference would be smaller than the likelihood of multiple responses to an direct query indicating that the reference exists. Experiments showed that indirect query approach works better and larger model are more capable and can hallucinate less.\\nAnti-Hallucination Methods#\\nLet’s review a set of methods to improve factuality of LLMs, ranging from retrieval of external knowledge base, special sampling methods to alignment fine-tuning. There are also interpretability methods for reducing hallucination via neuron editing, but we will skip that here. I may write about interpretability in a separate post later.\\nRAG → Edits and Attribution#\\nRAG (Retrieval-augmented Generation) is a very common approach to provide grounding information, that is to retrieve relevant documents and then generate with related documents as extra context.\\nRARR (“Retrofit Attribution using Research and Revision”; Gao et al. 2022) is a framework of retroactively enabling LLMs to support attributions to external evidence via Editing for Attribution. Given a model generated text $x$, RARR processes in two steps, outputting a revised text $y$ and an attribution report $A$ :\\n\\nResearch stage: Find related documents as evidence.\\n\\n(1) First use a query generation model (via few-shot prompting, $x \\\\to {q_1, \\\\dots, q_N}$) to construct a set of search queries ${q_1, \\\\dots, q_N}$ to verify all aspects of each sentence.\\n(2) Run Google search, $K=5$ results per query $q_i$.\\n(3) Utilize a pretrained query-document relevance model to assign relevance scores and only retain one most relevant $J=1$ document $e_{i1}, \\\\dots, e_{iJ}$ per query $q_i$.\\n\\n\\nRevision stage: Edit the output to correct content unsupported by evidence while preserving the original content as much as possible. Initialize the revised text $y=x$.\\n\\n(1) Per $(q_i, e_{ij})$, an agreement model (via few-shot prompting + CoT, $(y, q, e) \\\\to {0,1}$) checks whether the evidence $e_i$ disagrees with the current revised text $y$.\\n(2) Only if a disagreement is detect, the edit model (via few-shot prompting + CoT, $(y, q, e) \\\\to \\\\text{ new }y$) outputs a new version of $y$ that aims to agree with evidence $e_{ij}$ while otherwise minimally altering $y$.\\n(3) Finally only a limited number $M=5$ of evidence goes into the attribution report $A$.\\n\\n\\n\\n\\nFig. 12. Illustration of RARR (Retrofit Attribution using Research and Revision). (Image source: Gao et al. 2022)\\nWhen evaluating the revised text $y$, both attribution and preservation metrics matter.\\n\\nAttribution measures how much of $y$ can be attributed to $A$ using AIS (Attributable to Identified Sources) scores. We can collect human annotations or use a NLI model to approximate auto-AIS score.\\nPreservation refers to how much $y$ preserves the original text of $x$ , measured as $\\\\text{Prev}_\\\\text{intent} \\\\times \\\\text{Prev}_\\\\text{Lev}$, where $\\\\text{Prev}_\\\\text{intent}$ needs human annotation and $\\\\text{Prev}_\\\\text{Lev}$ is based on the character-level Levenshtein edit distance.\\nRARR leads to better-balanced results, especially in terms of preservation metrics, compared to two baselines.\\n\\nSimilar to RARR using search + editing, FAVA (“Factuality Verification with Augmented Knowledge”; Mishra et al. 2024) also retrieves relevant documents and then edits the model output to avoid hallucination errors. The FAVA model consists of a retriever $\\\\mathcal{M}_\\\\text{ret}$ and an editor $\\\\mathcal{M}_\\\\text{edit}$.\\n\\nGiven a prompt $x$ and model output $y$, the top relevant documents are retrieved: $d =  \\\\mathcal{M}_\\\\text{ret}(x, y)$\\nAn augmented output is generated by editor: $\\\\hat{y} = \\\\mathcal{M}_\\\\text{edit}(x, y, d)$\\n\\nRARR does not require training, but the editor model $\\\\mathcal{M}_\\\\text{edit}$ in FAVA needs to be fine-tuned. Following a more detailed taxonomy of categorizing different types of hallucination errors, we can generate synthetic training data for $\\\\mathcal{M}_\\\\text{edit}$  by inserting random errors into the model generation. Each example is a triplet $(c, y, y^*)$ where $c$ is the original Wikipedia paragraph as the gold context, $y$ is LM output with errors, and $y^∗$ is an output with error tags and correct editing.\\n\\nFig. 13. Synthetic data generation for training M_edit in FAVA. (Image source: Mishra et al. 2024)\\nRethinking with retrieval (RR; He et al. 2022) methods relies on retrieval of relevant external knowledge as well, but no additional editing. Instead of utilizing a search query generation model, RR’s retrieval is based on decomposed CoT prompting. Given an input prompt $Q$, RR uses CoT prompting to generate multiple reasoning paths ${R_1, \\\\dots, R_N}$  at temperature > 0, where each $R_i$ reasoning path contains an explanation $E_i$ (i.e. reasoning portion) followed by a prediction $P_i$ (i.e. the actual model output). The external knowledge $K_1, \\\\dots, K_M$ is retrieved to support each explanation. Then we select the most faithful answer $\\\\hat{P}$ based on how well it fits retrieved knowledge $K_1, \\\\dots, K_M$.\\n\\nKnowledge retrieval: RR’s experiments apply sparse retrieval BM25 against Wikipedia and then rerank by embedding cosine similarity provided by a pretrained MPNet model.\\nFaithfulness score: The faithfulness of each reasoning path is estimated by combining entailment scores, contradiction scores, and MPNet similarities. Both entailment and contradiction scores are provided by a pre-trained NLI model.\\n\\n\\nFig. 14. Performance of RR (Rethinking of retrieval) in comparison with other methods on commonsense reasoning (StrategyQA), temporal reasoning (TempQuestions) and tabular reasoning (INFOTABS) benchmarks, measured by the exact match metric. (Image source: He et al. 2022)\\nSelf-RAG (“Self-reflective retrieval-augmented generation”; Asai et al. 2024) trains a LM end-to-end to learn to reflect on its own generation by outputting both task output and intermittent special reflection tokens. They created a supervision dataset for a critic model and a generator model by prompting GPT-4 and then distilled that into an in-house model to reduce inference cost.\\n\\nFig. 15. Overview of Self-RAG framework. Guided by special tokens, Self-RAG model retrieves multiple documents in parallel and critiques its own generation to improve quality. (Image source: Asai et al. 2024)\\nGiven the input prompt $x$, the generated output $y$ consists of multiple segments (e.g. one segment is one sentence) $y=[y_1, \\\\dots, y_T]$. There are four type of reflection tokens in total, one for retrieval and three for critique:\\n\\nRetrieve: decides whether to run retrieval in parallel to get a set of documents; output values: {yes, no, continue}.\\nIsRel: whether the prompt $x$ and retrieved document $d$ relevant; output values: {relevant, irrelevant}.\\nIsSup whether the output text $y$ is supported by $d$; output values: {fully supported, partially supported, no support}.\\nIsUse: whether the output text $y$ is useful to $x$; output values: {5, 4, 3, 2, 1}.\\n\\nSelf-RAG generates one segment of $y_t$  at one time. Given $x$ and the proceeding generation $y_{<t}$, the model decodes the Retrieve token:\\n\\nIf Retrieve == no, generate $y_t$ directly;\\nIf Retrieve == yes, the model retrieves multiple passages in parallel and uses an IsRel token to check whether the retrieved document is relevant. If relevant, generate $y_t$ and use other critique tokens to score, rank and select the best among multiple outputs.\\n\\nChain of Actions#\\nWithout grounding by external retrieved knowledge, we can design a process for using the model itself to do verification and revision to reduce hallucination.\\nDhuliawala et al. (2023) proposed a method named Chain-of-Verification (CoVe) based on a chain of actions to plan and execute verification. CoVe consists of four core steps:\\n\\nBaseline response: The model produces an initial draft response, named “baseline”.\\nPlan verification: Based on this original generation, the model designs non-templated verification questions for fact checking; can be achieved by few-shot prompting with (response, verification questions) examples.\\nExecute verifications: The model answers those questions independently. There are a few variants of setups,\\n\\n(1) Joint: join with step 2, where the few-shot examples are structured as (response, verification questions, verification answers); The drawback is that the original response is in the context, so the model may repeat similar hallucination.\\n(2) 2-step: separate the verification planning and execution steps, such as the original response doesn’t impact\\n(3) Factored: each verification question is answered separately. Say, if a long-form base generation results in multiple verification questions, we would answer each question one-by-one.\\n(4) Factor+revise: adding a “cross-checking” step after factored verification execution, conditioned on both the baseline response and the verification question and answer. It detects inconsistency.\\n\\n\\nFinal output: Generate the final, refined output. The output gets revised at this step if any inconsistency is discovered.\\n\\nCoVe is designed this ways because using long-form chain-of-verification generation may result in repeated hallucination because the initial hallucinated response is still in the context and can be attended to during the new generation, whereas answering individual verification questions separately leads to better results than long-form generation.\\n\\nFig. 16. Overview of Chain-of-Verification (CoVe) method, running in four key steps.\\n (Image source: Dhuliawala et al. 2023)\\nHere are some interesting observations from the CoVe experiments:\\n\\nInstruction-tuning and CoT do not reduce hallucinations.\\nFactored and 2-step CoVe improve performance and further explicit reasoning on inconsistency detection also helps (“factor+revise” approach).\\nShort-form verification questions are more accurately answered than long-form queries.\\nFree-form LLM-generated verification questions are better than heuristics (e.g. Does X answer the question?) and  questions that require open-ended generation work better than yes/no questions.\\n\\nRECITE (“Recitation-augmented generation”; Sun et al. 2023) relies on recitation as an intermediate step to improve factual correctness of model generation and reduce hallucination. The motivation is to utilize Transformer memory as an information retrieval mechanism. Within RECITE’s recite-and-answer scheme, the LLM is asked to first recite relevant information and then generate the output. Precisely, we can use few-shot in-context prompting to teach the model to generate recitation and then generate answers conditioned on recitation. Further it can be combined with self-consistency ensemble consuming multiple samples and extended to support multi-hop QA.\\n\\nFig. 17. Comparison of direct generation, RAG and RECITE.(Image source: Sun et al. 2023)\\nThe generated recitation is comparable with the BM25 based retrieval model, but both have gaps with the use of ground truth passage. According to their error analysis, about 7-10% questions have the correct recitation but cannot produce the correct answer, while around 12% questions do not have the correct recitation but can be answered correctly anyway.\\nSampling Methods#\\nLee, et al. (2022) found that nucleus sampling (top-$p$ sampling) is found to perform worse on FactualityPrompt benchmark than greedy sampling, although it achieves better diversity and less repetition, since nucleus sampling added extra randomness. So they proposed factual-nucleus sampling algorithm, based on the hypothesis that sampling randomness does more harm to factuality at the latter part of the sentence than at the beginning. Factual-nucleus sampling is designed to dynamically adapt the probability $p$ during sampling tokens for each sentence. For the $t$-th token in one sentence, we have $p_t = \\\\max(\\\\omega, p \\\\cdot \\\\lambda^{t−1})$ where $\\\\omega$ is to prevent the sampling falls back to greedy that hurts generation quality and diversity.\\n\\nFig. 18. Factual-nucleus sampling leads to be better diversity and less repetition then the standard nucleus sampling, while the hallucination error is measured in named entity (NE) error. (Image source: Lee et al. 2022)\\nInference-Time Intervention (ITI; Li et al. 2023) investigated whether certain attention heads are more correlated with factuality by fitting a linear probe on the activations in each layer to discriminate between truthful vs false outputs. They found for many heads, the probes cannot do better than random, while some show strong performance. After identifying a sparse set of attention heads with high linear probing accuracy for truthfulness, at inference time ITI shifts activations of top $K$ selected attention heads along the “truthful” direction.\\n\\nFig. 19. Illustration of how activation is shifted on selected attention heads towards more truthfulness. (Image source: Li et al. 2023)\\nFine-tuning for Factuality#\\nLee, et al. (2022) proposed two ideas for factuality-enhanced training:\\n\\nTopicPrefix is introduced into training for better awareness of facts: Append topic (i.e. wikipedia document title) in front of each sentence in this document.\\nSentence completion loss as training objective: update the training loss to focus on the later part of the sentence where they hypothesize that the later part of a sentence contains more factual knowledge. The implementation is quite simple, deciding a pivot $t$, and all the tokens before the $t$-th token are all applied zero-masking. In their experiment, the best pivot $t$ is selected as 0.5 x the sentence length.\\n\\nLin et al. (2024) proposed to do run SFT + RLHF alignment training with special focus on factuality, named FLAME (“Factuality-Aware Alignment”).\\n\\nSFT stage (Factuality-aware SFT): The goal is to generate training data that is more factual (measured by FActScore) than the model’s own generation.\\nRLHF stage (Factuality-aware DPO): Two approaches are tested and the method (1) turns out pretty bad, while (2) works out ok, likely due to (1) trying to distill new knowledge into the model without enough training. There is evidence that fine-tuning new knowledge might cause hallucination and the supervision from RAG contains information unknown to the LLM.\\n\\n(1) Use the RAG data sample as positive and the original model generation as negative as RM data.\\n(2) Use FActScore as the reward signal on factuality.\\n\\n\\n\\n\\nFig. 20. Illustration of (Left) response generation using a pre-trained LLM with few-shot prompting and (Right) factuality-aware alignment training pipeline. (Image source: Lin et al. 2024)\\nTo avoid accidentally distilling unknown knowledge into the model during alignment training, they suggested using the model generated responses to form SFT / DPO datasets.\\n\\nFig. 21. Performance of SFT and DPO runs, with and without factuality-aware setup, on the task of biography generation. Helpfulness is measured by models\\' win rate over our baseline SFT + DPO on Alpaca Eval. Note that RLHF makes factuality worse, because human feedback often prefers longer, more detailed answers, which are not necessarily more factual. (Image source: Lin et al. 2024)\\nFactuality tuning (Tian & Mitchell et al. 2024) also relies on fine-tuning language models for better factuality. They experimented with different ways of truthfulness estimation of atomic claims in each model sample and then run DPO\\n\\nFig. 22. Illustration of factuality estimation process. (Image source: Tian & Mitchell et al. 2024)\\nProcess of factuality tuning:\\n\\nSample pairs of model completions for a given set of prompts (e.g \"Write a bio of Yo-Yo Ma\")\\nAnnotate them with truthfulness based on two methods without human involved:\\n\\nReference-based: check whether external knowledge base supports the model statement, similar to the above section on retrieval-based hallucination evaluation.\\n\\n(a) Extract a list of atomic claims;\\n(b) Find wikipedia reference;\\n(c) Use a small NLI fine-tuned model to check whether the reference text supports the atomic claim.\\n\\n\\nReference-free: use the model’s own confidence as a proxy of its truthfulness, similar to the indirect query approach.\\n\\n(a) Convert each claim into a corresponding question / need careful rephrase to ensure the question is unambiguous; using few-shot prompting;\\n(b) Sample multiple times from the model to answer that question;\\n(c) Compute the aggregated score / use string match or ask GPT to judge whether two answers are semantically equivalent.\\n\\n\\n\\n\\nConstruct a training dataset by generating multiple samples from the model and assign preference based on truthfulness scores. Then we fine-tune the model with DPO on this dataset.\\n\\n\\nFig. 23. Factuality tuning with FActScore (`FactTune-FS`) achieves the best improvement on factuality, compared to factuality tuning with expected confidence score (`FactTune-EC`) and other baselines. (Image source: Tian & Mitchell et al. 2024)\\nFine-tuning for Attribution#\\nAssigning attribution in the model outputs when generating conditions on search results is a good way to reduce hallucination. There is a branch of work to train LLMs to better consume retrieved content and assign high-quality attributions.\\nWebGPT (Nakano, et al. 2022) combines web search for document retrieval with a fine-tuned GPT model, aiming to answer long-form questions to reduce hallucination and achieve better factual accuracy. The model interacts with the Internet search in a text-based Web browser and learns to answer with references to web pages. While the model is browsing, one of the actions it can take is to quote an extract from the current page. When this is performed, the page title, domain name and extract are recorded to be used later as a reference. The center of WebGPT is to use references to assist humans to judge factual correctness.\\nThe model is first supervised fine-tuned on demonstrations of humans using the web-browsing environment to answer questions for behavior cloning. Comparison data is collected between two model-generated answers to the same question (each with their own set of references), where answers are judged for their factual accuracy, coherence, and overall usefulness. Reward model is used for RL training and best-of-n rejection sampling. RL training and best-of-n rejection sampling. In comparison, RL only introduces a small benefit and it is even smaller when rejection sampling is used.\\n\\nFig. 24.  RL training only introduces slight improvement over BC (behavior cloning) baseline, especially when best-of-n rejection sampling is used. (Image source: Nakano et al. 2022)\\nGopherCite (Menick et al. 2022) is quite similar to WebGPT on using search engine to create support materials and teaching models to provide references. Both run supervised fine-tuning for bootstrapping and both apply RL training from human preference. But different from WebGPT that depends on human demonstration for behavior cloning, GopherCite generates demonstrations via few-shot prompting and each generation uses context stuffing with relevant documents and then use reward model to score which ones are the best.\\n\\nFig. 25. Illustration of demonstration generation procedure with reranking. (Image source: Menick et al. 2022)\\nOne additional trick to avoid low quality response is to configure the model to decline to answer with a canned answer \"I don\\'t know\", decided by a global RM threshold, known as selective prediction.\\n\\nFig. 26. Preference vs human-written baselines. Ties are counted as half point on each side. (Image source: Menick et al. 2022)\\nThe empirical results on RL is similar to WebGPT in that RL only brings in limited improvement or no improvement when combined with rejection sampling.\\nAppendix: Evaluation Benchmarks#\\nHere is a list of datasets mentioned in this post.\\nTruthfulQA (Lin et al. 2021) is designed to measure how well a LLM can generate truthful responses. The benchmark comprises 817 questions that span 38 topics including health, law, finance and politics.\\nFactualityPrompt (Lee, et al. 2022) is a benchmark consisting of both factual and nonfactual prompts. It relies on Wikipedia documents or sentences as the knowledge base for factuality grounding.\\nSelfAware (Yin et al. 2023) contains 1,032 unanswerable questions across five categories and 2,337 answerable questions. Unanswerable questions are sourced from online forums with human annotations while answerable questions are sourced from SQuAD, HotpotQA and TriviaQA based on text similarity with unanswerable questions.\\nLongFact (Wei et al. 2024 ) is designed for checking long-form generation factuality. It consists of 2280 fact-seeking prompts that seek long-form responses on 38 manually curated topics\\nHaDes (Liu et al. 2021) is a benchmark for hallucination detection as a binary classification task. The dataset is created by perturbing Wikipedia text and human annotation.\\nFEVER (Fact Extraction and VERification) dataset contains 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. Each claim is classified as Supported, Refuted or NotEnoughInfo.\\nFAVABench (Mishra et al. 2024) is a benchmark for evaluating fine-grained hallucination. There are 200 information-seeking source prompts and 3 model responses per prompt, resulting in 600 responses in total. Each model response is manually labeled with fine-grained annotations on hallucination error types.\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jul 2024). Extrinsic Hallucinations in LLMs. Lil’Log. https://lilianweng.github.io/posts/2024-07-07-hallucination/.\\n\\nOr\\n@article{weng2024hallucination,\\n  title   = \"Extrinsic Hallucinations in LLMs.\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Jul\",\\n  url     = \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\"\\n}\\nReferences#\\n[1] Ji et al. “Survey of hallucination in natural language generation.” ACM Computing Surveys (2022)\\n[2] Gekhman et al. “Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?” arXiv preprint arXiv:2405.05904 (2024).\\n[3] Min et al. “FActScore: Fine-grained atomic evaluation of factual precision in long form text generation.” EMNLP 2023.\\n[4] Wei et al. 2024 “Long-form Factuality in LLMs” arXiv preprint arXiv:2403.18802 (2024).\\n[5] Chern et al. “FacTool: Factuality detection in generative AI - a tool augmented framework for multi-task and multi-domain scenarios.” arXiv preprint arXiv:2307.13528 (2023).\\n[6] Lin et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” ACL 2022.\\n[7] Yin et al. “Do Large Language Models Know What They Don’t Know?” ACL 2023.\\n[8] Kadavath et al. “Language Models (Mostly) Know What They Know” arXiv preprint arXiv:2207.05221 (2022).\\n[9] Agrawal et al. “Do language models know when they’re hallucinating references?” arXiv preprint arXiv:2305.18248 (2023).\\n[10] Lin et al. “Teaching Models to Learn Uncertainty in Words.” arXiv preprint arXiv:2205.14334 (2022).\\n[11] Gao et al. “RARR: Researching and Revising What Language Models Say, Using Language Models.” ACL 2023.\\n[12] He et al. “Rethinking with retrieval: Faithful large language model inference.” arXiv preprint arXiv:2301.00303 (2022).\\n[13] Asai et al. “Self-RAG: Learning to retrieve, generate and critique through self-reflection.” ICLR 2024.\\n[14] Mishra et al. “Fine-grained Hallucination Detection and Editing for Language Models.” arXiv preprint arXiv:2401.06855 (2024).\\n[15] Lee, et al. “Factuality Enhanced Language Models for Open-Ended Text Generation.” NeuriPS 2022.\\n[16] Manakul et al. “SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models.” EMNLP 2023.\\n[17] Li et al. “Inference-Time Intervention:  Eliciting Truthful Answers from a Language Model.” NeuriPS 2023.\\n[18] Chuang et al. “DoLa: Decoding by contrasting layers improves factuality in large language models.” ICLR 2024.\\n[19] Dhuliawala et al. “Chain-of-Verification Reduces Hallucination in Large Language Models.” arXiv preprint arXiv:2309.11495 (2023).\\n[20] Sun et al. “Recitation-Augmented Language Models.” ICLR 2023.\\n[21] Lin et al. “FLAME: Factuality-Aware Alignment for Large Language Models.” arXiv preprint arXiv:2405.01525 (2024).\\n[22] Tian & Mitchell et al. “Fine-tuning Language Models for Factuality.” ICLR 2024. (code)\\n[23] Nakano, Hilton & Balaji, et al. “WebGPT: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[24] Menick et al. “Teaching language models to support answers with verified quotes.” arXiv preprint arXiv:2203.11147 (2022).\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a579516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pdf reader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('LLm_evaluation.pdf')\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec01c30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='LREC-COLING 2024, pages 9340–9351\\n20-25 May, 2024. © 2024 ELRA Language Resource Association: CC BY-NC 4.0\\n9340\\nIs LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM\\non Automatic Paper Reviewing T asks\\nRuiyang Zhou1 Lu Chen1,2, B Kai Yu1,2, B\\n1 X-LANCE Lab, Department of Computer Science and Engineering\\nMoE Key Lab of Artificial Intelligence, SJTU AI Institute\\nShanghai Jiao T ong University , Shanghai, China\\n2 Suzhou Laboratory , Suzhou, China\\n{ellenruiyang,chenlusz,kai.yu}@sjtu.edu.cn\\nAbstract\\nThe use of large language models (LLM), especially ChatGPT , to help with research has come into practice.\\nResearchers use it for timely advice and hope to obtain in-depth feedback. However , can LLM be a qualified\\nand reliable reviewer? Although there already exist several review-related datasets, few works have carefully and\\nthoroughly inspected model’s capability as a reviewer , especially the correctness of generated reviews. In this\\npaper , we first evaluate GPT -3.5 and GPT -4 (the current top-performing LLM) on 2 types of tasks under different\\nsettings: the score prediction task and the review generation task. In addition, we propose a dataset containing\\n196 review-revision multiple-choice questions (RR-MCQ) with detailed labels from the review-rebuttal forum in\\nICLR-2023. By asking questions from technical details to the overall presentation and quality , our RR-MCQ data\\nprovides a more complete model ability assessment. The results show that LLM is generally helpful, but great\\ncaution is needed as it always makes mistakes. Although it can give passable decisions (> 60% accuracy) on single\\noptions, completely correct answers are still rare (about 20%); models are still weak on long paper processing,\\nzero-shot scoring, and giving critical feedback like human reviewers.\\nKeywords: automatic peer review, large language model, multiple choice question answering\\n1. Introduction\\nUtilizing large language models for scientific paper\\nreview recently attracts researcher’s interest. The\\ncontinuously growing amount of new paper pub-\\nlications, together with the increasing specializa-\\ntion within various research fields makes it a chal-\\nlenge to obtain timely and in-depth feedback. At\\nthe same time, LLM demonstrates strong ability\\nin reading comprehension, knowledge integration,\\nand even logical reasoning (OpenAI, 2023). Thus\\narises naturally this question: can LLM be a qual-\\nified and reliable automatic reviewer?\\nIn fact, even before the release of (truly) large lan-\\nguage models, there already exist datasets and\\nmethods targeting review-related tasks. For ex-\\nample, finetuning pretrained models to predict pa-\\nper decision and review scores (Li et al., 2020), or\\nusing language models to generate review texts\\n(Yuan et al. , 2022). Lately , new datasets for re-\\nview generation and edit generation also appear\\n(D’ Arcy et al., 2023), but there is still no detailed\\nassessment of model’s reviewing ability .\\nIn this paper , we first examine the reviewing abil-\\nity of GPT -3.5 and GPT -4 from two perspectives:\\nreview aspect score prediction and review genera-\\ntion. The two types of tasks evaluate both the abil-\\nity to discover flaws in research papers and rectify\\nthem, from the granularity level of abstract scor-\\ning to detailed commenting. We take great caution\\nB Lu Chen and Kai Yu are corresponding authors.\\nduring the evaluation process due to the innate dif-\\nficulty of evaluating freely generated texts: besides\\nclassical automatic metrics, new metrics and man-\\nual evaluations are also implemented.\\nWe then design a “qualification exam” for fine-\\ngrained analysis: we construct 196 review-\\nrevision-related multiple-choice questions. On pre-\\nvious datasets like review generation ones, de-\\ntailed analyses are only possible when manually\\nexamining the generated text, resulting in huge\\ntime costs and subjective conclusions. Even with\\nmanual analysis, the correctness of generated re-\\nviews is still diﬀicult to measure. In contrast, our\\nRR-MCQ dataset with well-defined categorization\\nlabels enables comprehensive and satisfactory as-\\nsessments. The questions are inferred from real\\ndiscussion forums of 55 reviews from 14 papers in\\nICLR-2023, investigating both criticizing and cor-\\nrecting abilities. Due to the high cost of designing\\nhigh-quality questions, we limit the total number of\\nquestions to about 200 (196 to be specific).1\\nWe come to the following conclusions:\\n• LLM has the potential to give meaningful\\nscores and decide on individual statements.\\n• However , they are NOT easy to use in prac-\\ntice: seldom fully correct, not critical enough,\\n1 Our RR-MCQ data is available at https:\\n//huggingface.co/datasets/zhouruiyang/\\nRR-MCQ'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='9341\\nlack technical details, and struggle with long\\ncontext.\\n• Automatic similarity metrics do not align with\\nthe true review generation quality; the assess-\\nment of model reliability is needed.\\n2. Related Work\\n2.1. Paper-reviewing Related T ask\\nGeneration task Automatic review generation is\\nthe most direct task in using models as automatic\\nreviewers. Datasets like PeerRead ( Kang et al. ,\\n2018), ASAP ( Yuan et al. , 2022), ReviewRobot\\n(Wang et al. , 2020), MOPRD ( Lin et al. , 2023),\\nand NLPEER (Dycke et al., 2022) all contain scien-\\ntific papers (mostly in the domain of computer sci-\\nence) and their corresponding peer reviews. How-\\never , since it is diﬀicult to directly generate review\\ntexts and evaluate them, various types of annota-\\ntions have been proposed. The most common la-\\nbel is the sentence type, classified based on the\\nsentence’s sentiment polarity , review aspect, or\\nthe text aspect that it comments on, for example\\nCOMP ARE (Singh et al., 2021), ReAct (Choudhary\\net al. , 2021), AMSR ( Fromm et al. , 2021), COM-\\nP ARE (Singh et al. , 2021), Peer-Review-Analyze\\n(Ghosal et al. , 2022), and AMPERE ( Hua et al. ,\\n2019). Still, there is no commonly recognized best\\nannotation style and evaluation metric.\\nOther generation tasks related to reviewing ability\\nalso appear , like meta-review generation and edit\\n(revision) generation. The meta-review generation\\ntask is more similar to the summarization task, but\\nit summarizes multiple peer reviews in the scien-\\ntific field. MreD ( Shen et al. , 2021) is an exempli-\\nfied meta-review generation task dataset with sen-\\ntence intent annotations. The revision generation\\ntask is more complicated, requiring the ability to\\ncomprehend the comment as well as take actions;\\nexamples are Revise and Resubmit ( Kuznetsov\\net al. , 2022), ArxivEdits ( Jiang et al. , 2022) and\\nARIES (D’ Arcy et al., 2023).\\nClassification task Besides directly generating\\ntexts, more specific tasks with clear-cut answers\\nare actually more investigated. Paper decision pre-\\ndiction and aspect score prediction are the two\\nmost researched tasks before the appearance of\\nlarge language models, like in PeerRead ( Kang\\net al., 2018). Other reviewing-related tasks include\\nargument extraction in RR (Cheng et al., 2020) and\\nDISAPERE (Kennard et al., 2021), sentence clas-\\nsification (on datasets mentioned above for review\\ngeneration with annotations).\\nIn this paper , we first evaluate models on both\\ntypes of existing tasks: the generation task and\\nthe classification task. Specifically , we choose to\\nrun GPT -3.5 and GPT -4 on the review generation\\ntask and aspect score prediction task; the result of\\nGPT -4 on edit generation is already presented in\\nARIES (D’ Arcy et al., 2023). We then present our\\nRR-MCQ data that inspects all aspects mentioned\\nabove.\\n2.2. Large Language Models for\\nReviewing\\nRecently ,Liu and Shah (2023) inspects GPT -4’s\\nability by constructing a small-sized artificial test\\ndataset. They first create 13 short papers and\\nthen design test examples based on these brand-\\nnew papers to avoid the data leakage problem.\\nThey find that GPT -4 can accomplish the list-\\nchecking task, but makes frequent mistakes on\\nerror-identifying and paper-ranking tasks. Their\\ndetailed analysis is only based on a limited number\\nof manually designed questions; in contrast, our\\nRR-MCQ dataset has more test questions whose\\ndistribution basically aligns with reality .\\nARIES ( D’ Arcy et al., 2023) proposes a dataset\\nfor comment-edit pairing and edit generation task,\\nbut find that even GPT -4 aligns badly the com-\\nment and the edit, and that the GPT -4 gener-\\nated revisions have low coherence and insuﬀi-\\ncient technical details. However , they do not mea-\\nsure the correctness of generated revisions as\\nit is extremely diﬀicult. Our work turns the free-\\ngeneration task into a multiple-choice question-\\nanswering task, making the measurement of cor-\\nrectness easy and automatic. It is like a qualifi-\\ncation test for LLM before being an automatic re-\\nviewer .\\nRobertson (2023) tests the usefulness of GPT -4\\ngenerated reviews by questioning 10 real users.\\nVery recently , Liang et al. (2023) assess on a\\nlarger scale the GPT -4 generated reviews: they\\ntag the comment overlap (hit rate and several other\\noverlap coeﬀicients) and survey the user satisfac-\\ntion to measure the review quality . They find that\\nthe reviews have satisfactory overlap and consis-\\ntency with human references, but can be non-\\ngeneric and emphasize different aspects. Com-\\nment overlap is an important indicator , but our RR-\\nMCQ data offers more evaluation perspectivesof\\nthe model’s ability and reliability .\\n3. T ask 1: Aspect Score Prediction\\n3.1. PeerRead Dataset\\nFor the task of aspect score prediction, we use the\\nICLR-2017 subset of the PeerRead dataset (Kang\\net al., 2018). This subset contains 1.3k manually\\nannotated aspect scores (ranging from 1 to 5 in-\\nclusive) for 427 oﬀicial reviews from ICLR-2017\\nconference. The manual annotations ensure the\\nfeasibility and consistency of the aspect score pre-\\ndiction task: aspects that are not discussed in the\\nreview have a special not discussed score label.\\nSee Figure 1 for a concrete example.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content=\"9342\\n1. accuracy↑ 2. |diff|↓ 3. Pearson↑ 4. Spearman↑ 5. Kendall’s tau↑\\nbaseline 1. most frequent score 0.404 0.966 0.333 0.340 0.297\\ngiven review\\n2. zero-shot 0.353 0.856 0.548 0.553 0.475\\n3. few-shot 0.306 1.132 0.651 0.659 0.580\\n4. MCQ style 0.336 1.025 0.558 0.565 0.492\\ngiven paper\\n5. abstract 0.237 0.992 0.228 0.233 0.195\\n6. whole paper (GPT -3.5-16k)0.138 2.132 0.131 0.131 0.109\\n7. selected sections 0.251 0.886 0.258 0.265 0.222\\n8. abstract & sections 0.330 0.923 0.248 0.249 0.209\\nT able 1: Average results of the aspect score prediction task from GPT -3.5 and GPT -3.5-16k on PeerRead\\ndataset. ↑ means the higher the metric value, the better the performance. The best result under each\\nsetting is bolded, and the best score across all settings is further italicized.\\n1. Recommendation2. Substance 3. Appropriateness 4. ComparisonP↑ Sp↑ K↑ P↑ Sp↑ K↑ P↑ Sp↑ K↑ P↑ Sp↑ K↑\\n1. zero-shot 0.826 0.836 0.7570.394 0.414 0.3670.473 0.489 0.4390.393 0.399 0.3492. few-shot 0.807 0.811 0.7330.453 0.452 0.4130.634 0.604 0.5580.405 0.401 0.353given review 3. MCQ style 0.819 0.824 0.7440.430 0.432 0.3820.393 0.453 0.3920.344 0.313 0.273\\n4. abstract 0.283 0.282 0.25 0.187 0.190 0.1660.187 0.152 0.129 -0.023-0.031-0.0285. whole paper (GPT -3.5-16k)0.090 0.091 0.0800.000-0.003-0.003-0.100-0.019-0.097-0.0300.030 -0.0256. selected sections-0.076-0.081-0.0720.155 0.131 0.1170.202 0.222 0.197 0.021 0.011 0.009given paper\\n7. abstract & sections-0.014-0.008-0.0070.0320.051 0.046 0.002 -0.007-0.0070.080 0.076 0.068\\n5. Soundness 6. Originality 7. Clarity 8. ImpactP↑ Sp↑ K↑ P↑ Sp↑ K↑ P↑ Sp↑ K↑ P↑ Sp↑ K↑\\n1. zero-shot 0.585 0.619 0.5420.507 0.510 0.4430.626 0.649 0.5720.445 0.450 0.3892. few-shot 0.667 0.674 0.6100.612 0.621 0.5450.730 0.745 0.6760.504 0.521 0.460given review 3. MCQ style 0.398 0.395 0.3550.476 0.469 0.4090.726 0.718 0.6440.494 0.493 0.436\\n4. abstract 0.120 0.117 0.1040.118 0.119 0.0980.172 0.171 0.1510.109 0.113 0.0975. whole paper (GPT -3.5-16k)0.052 0.0640.0570.0950.103 0.085-0.082-0.091-0.0810.069 0.086 0.0716. selected sections0.008 0.016 0.0140.197 0.189 0.1570.081 0.105 0.093 0.088 0.075 0.066given paper\\n7. abstract & sections0.157 0.183 0.1620.0840.079 0.067 0.112 0.1430.128 0.118 0.109 0.097\\nT able 2: Detailed aspect prediction results of GPT -3.5 and GPT -3.5-16k on PeerRead dataset. Numbers\\nin gray color are values with p-value larger than 0.05. P is short for Pearson correlation, Sp for Spearman\\ncorrelation, and K for Kendall’s tau.\\nThe exposition is OK, and I \\nthink the approach is sensible, \\nbut the main issue with this \\npaper is that it is lacking \\nexperiments on non-synthetic \\ndatasets. As such, while I find \\nthe graphics inspired ques-\\ntions the paper is investigating \\ninteresting, I don't think it is \\nclear that this work introduces \\nuseful machinery for modeling \\nmore general videos. I think \\nthis paper is more appropriate \\nas a workshop contribution in \\nits current form.\\nTitle: Perception Updating \\nNetworks: On Architectural \\nConstraints For Interpretable \\nVideo Generative Models\\nAbstract: We investigate a \\nneural network architecture and \\nstatistical framework that...\\nSection 1 Introduction: The \\ncurrent computer graphics \\npipelines... Section 3 \\nPerception Updating Networks: \\nThis Section proposes a family \\nof neural architectures for \\noptimizing...\\nSetting 2: Given Paper\\nAspect Score\\nSetting 1: Given Review\\nRecommendation = 2\\nSubstance = 2\\nAppropriateness = 2\\nComparison = not discussed\\nSoundness = 3 \\nOriginality = not discussed\\nClarity = not discussed\\nImpact = 3 \\nLLM\\nFigure 1: Example of the aspect score prediction\\ntask. We conduct experiments under two settings:\\ngiven the review or the paper to predict scores.\\nWe conduct experiments under two different set-\\ntings: (1) given human-written review, predict as-\\npect scores; (2) given (part of) the research paper ,\\naccuracy↑ |diff|↓ Pearson↑ Spearman↑ Kendall’s tau↑\\nmost freq0.317 0.813 0.628 0.630 0.560all 1 0.314 0.952 0.522 0.525 0.459all 5 0.337 0.822 0.546 0.539 0.476\\nT able 3: Average results of “only given abstract”\\nmethod on 100 randomly chosen examples from\\nPeerRead dataset. “most freq” means using the\\nmost frequent reference score for each aspect in\\nthe prompt example; similarly , “all 1” and “all 5”\\nmean setting all scores in the prompt example to 1\\nand 5.\\npredict scores.\\n3.2. Setting 1: Given Review, Predict\\nScores\\nT ests under the Setting 1 can be viewed as a re-\\nview reading comprehension task, targeting ques-\\ntions: are model-generated scores meaningful?\\nAre they consistent with the text? Can models\\nunderstand human-written reviews? In addition,\\nwe take into consideration the influence of prompt\\nstyle and content extraction methods. For Set-\\nting 1, we try zero-shot / few-shot, direct scoring\\n/ multiple-choice style scoring, and different exam-\\nple distributions.\\nBesides classical metrics accuracy and absolute\"),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content=\"9343\\ndifference for the score prediction task, we also cal-\\nculate the correlation indicators of Pearson, Spear-\\nman, and Kendall’s tau. The three correlation met-\\nrics are the common choice when evaluating the\\nscore prediction ability of unfinetuned models, as\\nin the work of using LLM to evaluate abstractive\\nsummaries (Shen et al., 2023) and machine trans-\\nlations (Kocmi and Federmann, 2023).\\nIf not specially marked, all models are of version\\n0613 with temperature 0.3, for example GPT -3.5-\\nturbo-0613 in this section.\\nLLM can infer scores from reviews. As shown\\nin T able 1 (column 3 Pearson), when predicting\\nscores given the review, GPT -3.5 achieves a good\\ncorrelation with humans (0.651 Pearson correla-\\ntion value under the few-shot setting, while the\\nbaseline is only about 0.3). Even under the most\\ndiﬀicult zero-shot setting (line 2 zero-shot & col-\\numn 345), its correlations are still satisfactory\\n(above 0.5). This indicates that GPT -3.5 can un-\\nderstand human-written reviews, distinguish emo-\\ntions, and give consistent scores.\\nAnother result worth noticing is that the multiple-\\nchoice question style prompting does not help\\nmuch (line 4 MCQ style). In the MCQ style prompt,\\nwe write specific scoring criteria for each score in\\neach aspect but only obtain a small performance\\ngain compared to zero-shot prompting. We may\\nconclude that GPT -3.5 already knows the rules\\nand can inherently give meaningful scores.\\n3.3. Setting 2: Given Paper, Predict\\nScores\\nExperiments under the Setting 2 really assess\\nmodel’s capability to be a reviewer , answering our\\nmain research question. Under this task setting,\\nwe try three types of input for LLM: only abstract,\\nselected sections, and the whole paper (for GPT -\\n3.5-16k).\\nLLM fails to predict scores directly from pa-\\npers. Unlike predicting scores given the review,\\nwhen only given (part of) the research paper , GPT -\\n3.5 struggles to generate reasonable scores. The\\nbottom half (line 5678) of T able1 shows that GPT -\\n3.5 only has 0.258 best Pearson correlation, even\\nlower than the baseline. The particularly poor per-\\nformance of GPT -3.5-16k (line 6) with correlations\\nlower than 0.2 gives us another indication: simply\\ninjecting long texts is not the way out, especially\\ncomplex long texts like research papers.\\nLLM predicts well the final [recommendation]\\nscore, but not scores of [comparison], [im-\\npact], and [substance] that are knowledge-\\nand logic-demanding. GPT -3.5 gains espe-\\ncially high correlations in predicting [Recommen-\\ndation] scores under both settings (Pearson corre-\\nlation 0.826 & 0.283, see detailed results for each\\naspect in T able2 column 1). However , it struggles\\nin judging [Comparison] (column 4) whether the pa-\\nper presents enough meaningful comparisons with\\nrelated work, [Substance] (column 2) whether it\\ncontains lots of ideas and results, and [Impact] (col-\\numn 8) whether it is influential and helpful to this\\nfield. We may attribute the diﬀiculty to the need of\\nextra scientific knowledge and details, as all three\\naspects require a rich understanding of the field.\\nHowever , we cannot exclude the possibility of us-\\ning memorized data to successfully predict the\\n[Recommendation] score (data leakage), as this\\nscore is the easiest to infer from other factors\\nand the PeerRead dataset uses ICLR-2017 pa-\\npers. Therefore, we present a more detailed exam-\\nination of model’s ability in Section 5 on our MCQ\\ntest data.\\nWe justify the choice of prompt example in T able3.\\nUsing the most frequent score of each aspect in\\nthe prompt has the best result, but the influence\\nis not decisive, as the variance among the three\\nprompts’ results is small. Therefore, we use the\\n“most frequent” score in the prompt example for all\\nexperiments in this section.\\n4. T ask 2: Review Generation\\n4.1. ASAP dataset\\nFor the task of review generation, we use the ICLR-\\n2020 subset of the ASAP dataset ( Yuan et al. ,\\n2022). Review sentences in this subset are la-\\nbeled by their aspect: summary , motivation, origi-\\nnality , soundness, substance, replicability , mean-\\ningful comparison, clarity ; each is further classi-\\nfied into positive and negative, except summary.\\nWe randomly select 300 papers from this subset\\nwith 902 corresponding oﬀicial peer reviews to test\\nmodel’s review generation ability . An example of\\nGPT -4 generated review is shown in Figure2.\\n[SUBSTANCE_POSITIVE] The authors' approach \\nto approximating the IBLagrangian through \\na second order Taylor expansion is innova-\\ntive. [ORIGINALITY_POSITIVE] The deriva-\\ntion of a lower bound that corresponds to \\nan PAC-Bayes prior is an original contri-\\nbution to this field. [SOUNDNESS_POSITIVE] \\nThe soundness of this study is demonstrat-\\ned through a number of experiments which \\nimply easy and hard cases where one can or \\ncannot prove generalization. [CLARITY_POS-\\nITIVE] The paper is well-structured and \\nclear, making complex concepts accessible \\nto readers. \\n[SUBSTANCE+]\\n[ORIGINALITY+]\\n[SOUNDNESS+]\\n[CLARITY+]\\nFigure 2: Example review generated by GPT -4.\\nThe sentence aspect label is part of the generation\\nand is put at the beginning of each sentence. A\\nspecial [None] label is added if the sentence does\\nnot belong to any other types.\"),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='9344\\n4.2. Experiments and Results\\nAs in the aspect score prediction test in Section 3,\\nwe try zero-shot / few-shot prompting and different\\ncontent extraction methods in the two-step genera-\\ntion experiment. We present both the performance\\nof GPT -3.5-turbo-0613 and GPT -4-0613 for the re-\\nview generation task. Besides, we focus more on\\nthe evaluation method for model-generated review\\ntexts.\\nWe examine the quality from the following perspec-\\ntives. (1) Aspect coverage, thanks to the aspect\\nannotations in ASAP dataset. (2) Similarity to\\nreference reviews, including the classical statisti-\\ncal methods ROUGE-1/2/L (Lin, 2004), the model-\\nbased method BertScore ( Zhang et al. , 2019),\\nthe task-based reference-free metric BLANC (Vasi-\\nlyev et al. , 2020), and using GPT -4 to score the\\nsimilarity as in the evaluation of abstractive sum-\\nmaries (Shen et al., 2023) and of translation quality\\n(Kocmi and Federmann, 2023). (3) Manual analy-\\nsis for similarity and informativeness (whether con-\\ntaining enough details) proposed by Yuan and Liu\\n(2022) in KID-Review.\\nHere are some implementation details. GPT -3.5\\nis used for 300 randomly selected papers from\\nASAP , whose results are auto-evaluated in Fig-\\nure 3 and T able 5; GPT -4 is used for 50 papers\\nand the results are manually examined in T able6.\\nThe reason for only choosing 50 papers for GPT -\\n4 is that, the generation is expensive and that the\\nmanual analysis has also a high cost. For the two-\\nstep generation experiment, the section extraction\\nstep outputs the union of contents that are selected\\nbased on each review (most papers have three\\nreference reviews), and the union of useful con-\\ntents is input into the model to generate one re-\\nview. During the evaluation, the best similarity\\nscore is selected among all references for ROUGE\\nand BertScore; for BLANC, all reference reviews\\nare concatenated to form the new reference.\\nLLM has its own comment aspect preference:\\ntoo much positive feedback. In Figure 3, it is\\nclear that GPT -3.5 has its own preference of as-\\npects to comment on: it always generates positive\\nreviews while being very cautious about negative\\naspects. For each setting, the proportion of posi-\\ntive feedback (exclude [Summary]) is always larger\\nthan 55% (reference labels are only 43% positive).\\nEspecially when only given the abstract, 99% com-\\nments are positive. In addition, [Substance-] and\\n[Clarity-] are always missing in all four experiment\\nsettings.\\nFew-shot prompting helps GPT -3.5 to generate\\nmore negative comments, but the distribution is\\nstill very different from reality . We regard this as\\na strong weakness for GPT -3.5 being a reviewer\\nbecause well-supported critics are the most help-\\nful for researchers.\\nFigure 3: Aspect label distribution of reference\\nreviews and model-generated reviews on ASAP\\ndataset. “+ / -” means positive/ negative com-\\nments, except that [summary] is always neutral.\\nReference label distribution comes from the union\\nof all reviews for each paper .\\nThe aspect recall value in T able 5 (column 1 as-\\npect) also indicates that both GPT -3.5 and GPT -4\\nhave very different comment preferences from hu-\\nmans. Although their best aspect coverage rate\\n(0.582) is better than humans (0.499), the highest\\naspect recall is still unsatisfactory (0.559). Even\\nunder the few-shot prompt setting (line 2 few-shot),\\nthis value is still low (0.506), showing that LLM can-\\nnot naturally generate comments of people’s inter-\\nest.\\nAutomatic metrics can be fooled by LLM’s gen-\\neration. In T able4, to better examine the review\\ngeneration quality , we try using GPT -4 as an eval-\\nuator to predict the relevance, precision, and re-\\ncall score (between 0 and 100). We also manu-\\nally score (also between 0 and 100) the 50 reviews\\ngenerated by GPT -4-0613 when given both the ab-\\nstract and selected sections. We focus on two per-\\nspectives for quality evaluation: (1) relevance to\\nreference reviews; (2) amount of details in the re-\\nview (informativeness).\\nDuring the process of manual inspection, we find\\nthat GPT -4 generated reviews are strongly influ-\\nenced by the given paper content and lack crit-\\nics, while reference reviews contain lots of detailed\\nsuggestions. This is why the manual score is low,\\n54.5 for relevance and 48.7 for informativeness.\\nHowever , the manual score (and also our actual im-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='9345\\nGPT -4 predictedrelevance↑ precision↑ recall↑\\n83.8 84.7 75.8\\nmanually scoredrelevance↑ informativeness↑\\n54.5 48.7\\nT able 4: “GPT -4 as review quality evaluator” pre-\\ndicted scores and manually annotated scores for\\nthe 50 GPT -4 generated reviews.\\npression) is very different from BertScore (always\\nabove 0.8): is BertScore fooled by the fluent gener-\\nation? Thus, we compute the correlation between\\nevery automatically computed similarity score and\\nour manual annotations in T able 6. As expected,\\nBertScore has the worst correlation (-0.003 and -\\n0.024), showing that BertScore is fooled by GPT -4\\ngenerated texts.\\nAutomatic metrics that best correspond to humans\\nare BLANC and GPT -4. BLANC is designed for no-\\nreference summary evaluation, but here we con-\\nsider the generated review as a summary of all\\nreference reviews. BLANC uses the performance\\ngain on the blank-filling task as the indicator: if a\\nsummary can help its model to complete the origi-\\nnal task, then the summary is considered of good\\nquality . This metric truly tests the amount of infor-\\nmation contained in the summary and is diﬀicult to\\nbe fooled by fluent but hollow sentences.\\nIn conclusion, GPT -4 does not naturally provide\\nenough details and critics; automatic similarity met-\\nrics can be unaware of the flaws. Since it is still dif-\\nficult to evaluate model-generated reviews, we use\\nour RR-MCQ data in Section 5, a more objective\\nand detailed approach to evaluation.\\n5. T ask 3: Review-Revision\\nMultiple-Choice Questions\\nThe experiments of aspect score prediction and\\nreview generation above do give us an idea of\\nmodel’s reviewing ability . Moreover , GPT -4 has\\nalso been tested on the edit generation task in\\nARIES ( D’ Arcy et al. , 2023). They manually\\nannotate 85 generated examples from the fol-\\nlowing perspectives: compliance (1-3), promise\\n(true/false), paraphrase (true/false), and technical\\ndetails (true/false). They find that GPT -4 has high\\ncompliance (94% are scored 3), but tends to make\\npromises (21%), simply paraphrase the comment\\n(48%), and lacks technical details (12%).\\nOne evident drawback of the previous analyses is\\nthat, all detailed results require heavy manual in-\\nspection while automatic metrics are not reliable\\nenough (see Section 4.2). Therefore, we propose\\na review-revision multiple-choice question dataset\\n(RR-MCQ) with abundant labels, containing ques-\\ntions related to both the review and revision with\\none or more correct answers.\\n5.1. RR-MCQ Data Construction and\\nCharacteristics\\nQuestion:\\nWhat experiments could be added to illustrate \\nthe proposed method’s effectiveness?\\nA. change the similarity threshold\\nB. use the same examples for different models\\nC. use randomly sampled examples for \\ndifferent models\\nD. do not use the similarity threshold\\nOptions:\\nAnswers: A B\\nLabels: [soundness] [empirical] [add] [no-need]\\nFigure 4: Example of the multiple-choice question\\nwith one or more answers. We randomly shuffle\\nthe four options during the experiment.\\nOur RR-MCQ dataset is targeted for a more spe-\\ncific and in-depth assessment. For example, can\\nmodels evaluate the soundness of argumentation?\\nCan they integrate domain knowledge and the pa-\\nper together? Can they give complicated sugges-\\ntions, such as important experiments to do?\\nThe dataset contains 196 multiple-choice ques-\\ntions examining specific review-revision-related\\nknowledge and ability . An example of the RR-\\nMCQ is presented in Figure 4.\\nT o construct the MCQ test dataset, we select 55\\nreviews from 14 papers with suﬀicient comment-\\nresponse posts in the peer review forum from the\\nICLR-2023 conference. We then perform the fol-\\nlowing four steps: (1) align the smallest unit of com-\\nment and response to form a single argument; (2)\\nidentify its main topic and decide if controversial\\n(we skip arguments on which the two sides sharply\\ndisagree); (3) transform the argument into a four-\\nchoice question without adding new contents, i.e.\\nwrong options either come from irrelevant parts of\\nthe same discussion or are the negation of correct\\nones; (4) label the aspects being assessed by the\\nquestion.\\nThere are four types of labels corresponding to\\nfour ways of categorization: review aspect, con-\\ntent aspect, ability to be tested, if need informa-\\ntion from other papers. Figure 5 presents the la-\\nbel distribution, which basically corresponds to the\\nreview comment distribution in reality . The labels\\nare assigned by two experienced students in the\\ndomain. Among all the 788 annotated labels, 86\\nlabels (10.9%) have disagreement at first. The fi-\\nnal decision is made through a careful discussion\\nof the two annotators.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='9346\\n1. aspect (macro avg) 2. ROUGE-F1 (macro avg)\\nref coverage↑ coverage↑ recall ↑ R-1 ↑ R-2 ↑ R-L ↑\\nabstract 1. zero-shot 0.499 0.034 0.065 0.429 0.103 0.190\\n2. few-shot 0.499 0.582 0.506 0.424 0.108 0.198\\npaper 3. selected sections 0.499 0.367 0.449 0.382 0.081 0.176\\n4. abstract & sections 0.499 0.470 0.559 0.431 0.106 0.193\\nGPT -4* 5. abstract & sections 0.488* 0.515* 0.530* 0.425* 0.100* 0.190*\\n3. BertScore (macro avg) 4. BLANC\\nprecision↑ recall ↑ F1 ↑ average↑ variance\\nabstract 1. zero-shot 0.855 0.835 0.843 0.098 0.001\\n2. few-shot 0.851 0.839 0.843 0.114 0.001\\npaper 3. selected sections 0.843 0.832 0.835 0.093 0.001\\n4. abstract & sections 0.845 0.840 0.841 0.126 0.001\\nGPT -4* 5. abstract & sections 0.851* 0.840* 0.843* 0.112* 0.001*\\nT able 5: Automatic evaluation of GPT -3.5 (300 examples) and GPT -4 (50 examples) generated reviews\\non ASAP dataset. GPT -4 results are noted with * because they are averaged over 50 examples.\\nrelevance informativeness\\naspect-recall 0.076 0.227\\nRouge1-F1 0.115 0.159\\nRouge2-F1 0.039 0.111\\nRougeL -F1 0.167 0.124\\nBertScore-F1 -0.003 -0.024\\nBLANC-avg 0.255 0.355\\nGPT4-avg 0.325 0.244\\nT able 6: Pearson correlation values between auto-\\nmatic evaluation metrics and manually annotated\\nreview quality labels for the 50 GPT -4 generated\\nreviews.\\nFigure 5: Label distribution of our RR-MCQ test\\ndata. There are 4 types of labels: review as-\\npect, content aspect, ability , and if need informa-\\ntion from other papers.\\n5.2. Experiments and Results\\nWe test both GPT -3.5-turbo-0613 and GPT -4-0613\\non our MCQ data. The two-step generation\\nmethod is similar to that of Section 4: the model\\nselects useful sections based on the given ques-\\ntion, then the selected contents are input into the\\nmodel to predict answers. Note that our multiple-\\nchoice questions may have more than one correct\\nanswer .\\nLLM gets passable micro accuracy, but bad\\nmacro accuracy. From T able7, the best micro\\naccuracy 0.710 comes from the GPT -4 -> GPT -4\\nmethod. It is a passable score, but the macro ac-\\ncuracy is not ideal: the best macro accuracy is only\\n0.276. The micro accuracy is calculated by consid-\\nering each question option as an individual exam-\\nple so that the MCQ task becomes a binary de-\\ncision task of determining T rue/False for each op-\\ntion. The macro accuracy is more strict: only when\\nall answers are correct, the question is marked as\\ncorrect (note that the number of correct options is\\nundetermined, between one and four).\\nIn addition, the precision and recall are balanced.\\nWe can conclude that GPT -4 is able to judge\\nthe correctness of each individual statement with\\nabout 70% accuracy , but this level of ability is in-\\nsuﬀicient for giving completely correct answers: er-\\nrors and omissions are frequent.\\nLLM struggles with tasks that relate to sound-\\nness and adding components. We select the\\ntop 2 numerous labels in each of the 4 label cate-\\ngories and present the detailed results in T able 8.\\nAs our questions are inferred from the true peer\\nreview discussion forum (containing both the re-\\nview and the response), these types of questions\\nare also the most common in reality . Therefore,\\nmodel’s performance on these questions is impor-\\ntant and representative.\\nIn T able 8, [Explain] related questions have the\\nhighest macro accuracy (by comparing the bold\\nnumber in “accuracy” through all eight aspects).\\nThese types of questions come from discussions\\nto confirm an understanding or ask a detail, some-\\nhow similar to reading comprehension and essay\\nexpansion tasks. It does not require much logi-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='9347\\nmacro average micro average\\naccuracy↑ precision↑ recall↑ F1 ↑ accuracy↑ precision↑ recall↑ F1 ↑\\nGPT -3.5 -> GPT -3.5 0.128 0.332 0.376 0.176 0.569 0.583 0.373 0.227\\nGPT -4 -> GPT -3.5 0.214 0.553 0.586 0.285 0.648 0.644 0.603 0.311\\nGPT -4 -> GPT -4 0.276 0.655 0.666 0.330 0.710 0.699 0.701 0.350\\nT able 7: Results on RR-MCQ test data. “GPT -4 -> GPT -3.5” means using GPT -4 for the first section\\nselection step and then using GPT -3.5 for the second answer generation step.\\n1. Soundness 2. Clarity 3. Empirical 4. Method\\nmacro average acc↑ prec↑ recall↑ acc↑ prec↑ recall↑ acc↑ prec↑ recall↑ acc↑ prec↑ recall↑\\nGPT -3.5 -> GPT -3.50.091 0.293 0.307 0.194 0.356 0.454 0.055 0.267 0.308 0.173 0.391 0.458\\nGPT -4 -> GPT -3.50.205 0.526 0.552 0.278 0.498 0.560 0.209 0.573 0.597 0.247 0.564 0.598\\nGPT -4 -> GPT -40.193 0.655 0.673 0.361 0.514 0.509 0.253 0.713 0.695 0.309 0.617 0.654\\n5. Explain 6. Add 7. No need 8. Need\\nmacro average acc↑ prec↑ recall↑ acc↑ prec↑ recall↑ acc↑ prec↑ recall↑ acc↑ prec↑ recall↑\\nGPT -3.5 -> GPT -3.50.182 0.374 0.384 0.051 0.284 0.336 0.118 0.295 0.358 0.140 0.380 0.400\\nGPT -4 -> GPT -3.50.247 0.573 0.534 0.203 0.586 0.609 0.227 0.523 0.549 0.198 0.591 0.634\\nGPT -4 -> GPT -40.364 0.667 0.626 0.153 0.694 0.757 0.291 0.602 0.647 0.256 0.724 0.691\\nT able 8: Detailed results on RR-MCQ test data. For each label category , we present only the detailed\\nresults for the top two labels with the most elements.\\ncal reasoning, but lots of knowledge and inference\\nability .\\nQuestion [Soundness]:\\nWhich choice the author makes may hurt the \\nfinal performance but is not well-examined in \\nthe paper?\\nA. linearize the RNN module\\nB. use a learnable gate\\nC. combine RNN and self-attention\\nD. REM endows positional encodings of a \\nmulti-head self-attention with recurrent \\ndynamics\\nOptions:\\nAnswers: A\\nFigure 6: Example question of label [Soundness].\\nThe lowest macro accuracy appears in questions\\nof labels [Soundness] and [Add] (note that one\\nquestion may have both the [Soundness] and\\n[Add] labels because they mark different perspec-\\ntives).\\nAn example of [Soundness] question is shown in\\nFigure 6. The review aspect label [Soundness]\\nconcerns questions requiring strong logic, for ex-\\nample, the correctness of a statement, the validity\\nof an argument, or the completeness of supporting\\nevidence. In the example above, the model needs\\nto first identify whether the choice is influential, and\\nthen decide if it is well-examined in the paper .\\nAn example of [Add] question is shown in Figure7.\\nQuestion [Add]:\\nWhat possible experiments can be added to \\npresent the method’s usefulness ?\\nA. compare to linear freezing and AutoFreeze\\nB. try DeiT-T model on ImageNet dataset\\nC. try Bert model on MRPC dataset\\nD. try Bert model on CoLA dataset\\nOptions:\\nAnswers: B C D\\nFigure 7: Example question of label [Add].\\nThe tested ability label [Add] relates to adding com-\\nponents to the paper , for example, conducting an\\nextra experiment or citing a missing related work.\\nIts diﬀiculty comes from the need for both logic\\nand knowledge. Although possible options to be\\nadded are already provided in the question, the\\nmodel still has to carefully select truly necessary\\nones. In the example question above, GPT -4 fails\\nto understand that the first option is already in the\\npaper and that the second option is needed to\\nprove method’s effectiveness on other model ar-\\nchitectures.\\n6. Conclusion\\n“Can LLM be a qualified and reliable automatic re-\\nviewer?” After testing GPT -3.5 and GPT -4 on two\\nexisting datasets and also on our proposed RR-\\nMCQ data, we conclude that they are not natu-\\nrally reliable automatic reviewers because their er-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='9348\\nror rate is still not suﬀiciently low.\\nThey can generate meaningful scores based on\\nhuman-written reviews, even without explicitly giv-\\ning examples or scoring criteria; but when the in-\\nput is long and complicated like a whole research\\npaper , they can only roughly identify the quality .\\nWhen being asked to freely generate comments,\\ntheir suggestions are sometimes correct, but al-\\nways on aspects that human reviewers would not\\nbe interested in. Facing multiple-choice questions,\\nthey have the ability to make passable decisions\\non single options, but hardly to be completely cor-\\nrect .\\nWe claim that it is still too early to trust LLM as auto-\\nmatic scientific paper reviewer . Although there is a\\nchance to get useful and correct results, their cur-\\nrent capability is not reliable enough. Especially on\\nquestions requiring logic reasoning over long texts\\nor extra knowledge in detail, their performance is\\nstill unsatisfactory .\\nWe believe that in detail and in-depth evaluations\\nare needed before the targeted development of\\nLLM in automatic paper reviewing task. Our RR-\\nMCQ test dataset is an example, but still with lim-\\nited size. Future work could be to develop bet-\\nter data construction methods, to invent new met-\\nrics, and finally to improve LLM’s capability as a\\nreviewer .\\n7. Acknowledgements\\nThis work was supported by the National Key\\nR&D Program of China 2023ZD0120703 and the\\nChina NSFC Projects (U23B2057, 62106142\\nand 62120106006) and Shanghai Munici-\\npal Science and T echnology Major Project\\n(2021SHZDZX0102).\\n8. Bibliographical References\\nLiying Cheng, Lidong Bing, Qian Yu, Wei Lu, and\\nLuo Si. 2020. Ape: Argument pair extrac-\\ntion from peer review and rebuttal via multi-task\\nlearning. In Proceedings of the 2020 Confer-\\nence on Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 7000–7011.\\nGautam Choudhary , Natwar Modani, and Ni-\\ntish Maurya. 2021. React: A re view com-\\nment dataset for act ionability (and more). In\\nWeb Information Systems Engineering–WISE\\n2021: 22nd International Conference on Web\\nInformation Systems Engineering, WISE 2021,\\nMelbourne, VIC, Australia, October 26–29,\\n2021, Proceedings, Part II 22 , pages 336–343.\\nSpringer .\\nMike D’ Arcy , Alexis Ross, Erin Bransom, Bailey\\nKuehl, Jonathan Bragg, T om Hope, and Doug\\nDowney . 2023. Aries: A corpus of scientific\\npaper edits made in response to peer reviews.\\narXiv preprint arXiv:2306.12587.\\nNils Dycke, Ilia Kuznetsov , and Iryna Gurevych.\\n2022. Nlpeer: A unified resource for the com-\\nputational study of peer review. arXiv preprint\\narXiv:2211.06651.\\nMichael Fromm, Evgeniy Faerman, Max Berren-\\ndorf, Siddharth Bhargava, Ruoxia Qi, Y ao\\nZhang, Lukas Dennert, Sophia Selle, Y ang Mao,\\nand Thomas Seidl. 2021. Argument mining\\ndriven analysis of peer-reviews. In Proceedings\\nof the AAAI Conference on Artificial Intelligence,\\nvolume 35, pages 4758–4766.\\nTirthankar Ghosal, Sandeep Kumar , Prabhat Ku-\\nmar Bharti, and Asif Ekbal. 2022. Peer review\\nanalyze: A novel benchmark resource for com-\\nputational analysis of peer reviews. Plos one ,\\n17(1):e0259238.\\nXinyu Hua, Mitko Nikolov , Nikhil Badugu, and\\nLu Wang. 2019. Argument mining for un-\\nderstanding peer reviews. arXiv preprint\\narXiv:1903.10104.\\nChao Jiang, Wei Xu, and Samuel Stevens. 2022.\\narxivedits: Understanding the human revision\\nprocess in scientific writing. arXiv preprint\\narXiv:2210.15067.\\nDongyeop Kang, Waleed Ammar , Bhavana Dalvi,\\nMadeleine Van Zuylen, Sebastian Kohlmeier ,\\nEduard Hovy , and Roy Schwartz. 2018. A\\ndataset of peer reviews (peerread): Collection,\\ninsights and nlp applications. arXiv preprint\\narXiv:1804.09635.\\nNeha Kennard, Tim O’Gorman, Rajarshi Das, Ak-\\nshay Sharma, Chhandak Bagchi, Matthew Clin-\\nton, Pranay Kumar Y elugam, Hamed Zamani,\\nand Andrew McCallum. 2021. Disapere: A\\ndataset for discourse structure in peer review\\ndiscussions. arXiv preprint arXiv:2110.08520.\\nT om Kocmi and Christian Federmann. 2023. Large\\nlanguage models are state-of-the-art evalua-\\ntors of translation quality . arXiv preprint\\narXiv:2302.14520.\\nIlia Kuznetsov , Jan Buchmann, Max Eichler , and\\nIryna Gurevych. 2022. Revise and resubmit:\\nAn intertextual model of text-based collabora-\\ntion in peer review. Computational Linguistics,\\n48(4):949–986.\\nJiyi Li, Ayaka Sato, Kazuya Shimura, and Fumiyo\\nFukumoto. 2020. Multi-task peer-review score\\nprediction. In Proceedings of the First Workshop\\non Scholarly Document Processing, pages 121–\\n126.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='9349\\nWeixin Liang, Yuhui Zhang, Hancheng Cao,\\nBinglu Wang, Daisy Yi Ding, Xinyu Y ang, Kailas\\nVodrahalli, Siyu He, Daniel Scott Smith, Yian\\nYin, et al. 2023. Can large language models\\nprovide useful feedback on research papers?\\na large-scale empirical analysis. arXiv preprint\\narXiv:2310.01783.\\nChin- Y ew Lin. 2004. Rouge: A package for auto-\\nmatic evaluation of summaries. In T ext summa-\\nrization branches out, pages 74–81.\\nJialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong\\nChen, and Xiaodong Shi. 2023. Moprd: A mul-\\ntidisciplinary open peer review dataset. Neural\\nComputing and Applications, pages 1–16.\\nRyan Liu and Nihar B Shah. 2023. Reviewergpt?\\nan exploratory study on using large language\\nmodels for paper reviewing. arXiv preprint\\narXiv:2306.00622.\\nOpenAI. 2023. Gpt-4 technical report.\\nZachary Robertson. 2023. Gpt4 is slightly helpful\\nfor peer-review assistance: A pilot study . arXiv\\npreprint arXiv:2307.05492.\\nChenhui Shen, Liying Cheng, Y ang Y ou, and Li-\\ndong Bing. 2023. Are large language models\\ngood evaluators for abstractive summarization?\\narXiv preprint arXiv:2305.13091.\\nChenhui Shen, Liying Cheng, Ran Zhou, Li-\\ndong Bing, Y ang Y ou, and Luo Si. 2021.\\nMred: A meta-review dataset for structure-\\ncontrollable text generation. arXiv preprint\\narXiv:2110.07474.\\nShruti Singh, Mayank Singh, and Pawan Goyal.\\n2021. Compare: a taxonomy and dataset of\\ncomparison discussions in peer reviews. In\\n2021 ACM/IEEE Joint Conference on Digital Li-\\nbraries (JCDL), pages 238–241. IEEE.\\nOleg Vasilyev , Vedant Dharnidharka, and John\\nBohannon. 2020. Fill in the blanc: Human-\\nfree quality estimation of document summaries.\\narXiv preprint arXiv:2002.09836.\\nQingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight,\\nHeng Ji, and Nazneen Fatema Rajani. 2020. Re-\\nviewrobot: Explainable paper review generation\\nbased on knowledge synthesis. arXiv preprint\\narXiv:2010.06119.\\nWeizhe Yuan and Pengfei Liu. 2022. Kid-review:\\nKnowledge-guided scientific review generation\\nwith oracle pre-training. In Proceedings of the\\nAAAI Conference on Artificial Intelligence , vol-\\nume 36, pages 11639–11647.\\nWeizhe Yuan, Pengfei Liu, and Graham Neu-\\nbig. 2022. Can we automate scientific review-\\ning? Journal of Artificial Intelligence Research ,\\n75:171–212.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\\nWeinberger , and Y oav Artzi. 2019. Bertscore:\\nEvaluating text generation with bert. arXiv\\npreprint arXiv:1904.09675.\\nA. Prompt\\nA.1. Evaluation on PeerRead\\nSetting 1 Given review, predict scores.\\nPrompt Y ou are a professional reviewer in com-\\nputer science and machine learning. Based on\\nthe given review, you need to predict the review\\nscore in several aspects. Choose a score from\\n[1,2,3,4,5], higher score means better paper qual-\\nity .\\nZero-Shot Example Example output: RECOM-\\nMENDA TION: x, SUBST ANCE: x, APPROPRI-\\nA TENESS: x, MEANINGFU COMP ARISON: x,\\nSOUNDNESS CORRECTNESS: x, ORIGINAL -\\nITY : x, CLARITY : x, IMP ACT : x\\nFew-Shot Example Example1: Review: This\\npaper presents an approach to modeling videos\\nbased on a decomposition into a background ...\\nworkshop contribution in its current form. Output:\\nRECOMMENDA TION: 2, SUBST ANCE: 2, AP-\\nPROPRIA TENESS: 2, SOUNDNESS CORRECT -\\nNESS: 3, IMP ACT : 3 . Example2 ... Example5 ...\\nMCQ-Style Example RECOMMENDA TION: A.\\nThis paper changed my thinking on this topic and\\nI’d fight to get it accepted; B. I learned a lot from this\\npaper and would like to see it accepted. C. Border-\\nline: I am ambivalent about this one. D. Leaning\\nagainst: I would rather not see it in the conference.\\nE. Poor: I would fight to have it rejected.\\nSetting 2 Given paper , predict scores.\\nPrompt Y ou are a professional reviewer in com-\\nputer science and machine learning. Based on\\nthe given abstract/sections/paper , you need to pre-\\ndict the review score in several aspects. Choose a\\nscore from [1,2,3,4,5], higher score means better\\npaper quality .\\nA.2. Evaluation on ASAP\\nSetting 1 Given paper , generate review text.\\nPrompt Y ou are a professional reviewer in com-\\nputer science and machine learning. Based\\non the given title and abstract of a research\\npaper , you need to write a review in ICLR\\nstyle. At the same time, you need to tag\\nsequences of words with their review type'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='9350\\nat the beginning: [NONE], [SUMMARY], [MO-\\nTIV A TION POSITIVE], [[MOTIV A TION NEGA-\\nTIVE]], [SUBST ANCE POSITIVE], [SUBST ANCE\\nNEGA TIVE], [ORIGINALITY POSITIVE], [ORIGI-\\nNALITY NEGA TIVE], [SOUNDNESS POSITIVE],\\n[SOUNDNESS NEGA TIVE], [CLARITY POSI-\\nTIVE], [CLARITY NEGA TIVE], [REPLICABILITY\\nPOSITIVE], [REPLICABILIT NEGA TIVE], [MEAN-\\nINGFUL COMP ARISON POSITIVE], [MEANING-\\nFUL COMP ARISON NEGA TIVE]. Y our total output\\nshould not surpass 500 tokens.\\nZero-Shot Example Example output: [LABEL]\\nsequence. [LABEL] sequence. [LABEL] se-\\nquence......\\nFew-Shot Example Example1: [SUM-\\nMARY]This paper introduces a method to\\ndisentanglement the private and public attribute\\ninformation... Example2: [SUMMARY]The paper\\nproposes learning NN to correct for inaccuracies...\\nExample3: [SUMMARY]This paper describes a\\nmethod for segmenting 3D point clouds... Exam-\\nple4: [SUMMARY]This work introduces GQ-Net ,\\na novel technique that trains quantization friendly\\nnetworks...\\nSetting 2 Given reference reviews, evaluate the\\ngenerated review quality .\\nPrompt Score the following review step by step\\nwith respect to its relevance with reference reviews\\non a continuous scale from 0 to 100. Y ou should\\ngive a relevance score, a precision score and a\\nrecall score of the review to be scored. Rele-\\nvance measures its selection of important content\\nfrom references, where relevance=0 means ’no\\nmeaning preserved’ and relevance=100 means\\n’perfect meaning’. Precision measures its correct-\\nness with respect to references, and recall mea-\\nsures its information coverage with respect to ref-\\nerences. Output format: Score for the review to be\\nscored:relevance=x, precision=x, recall=x.\\nA.3. Evaluation on RR-MCQ\\nSetting 1 Given question and paper , select use-\\nful sections.\\nPrompt Y ou are a professional reviewer in ley-\\nwords. Y ou will be given a multiple choice ques-\\ntion and the headings of a research paper in this\\nfield. Y ou need to select sections that are useful\\nto anwer the question.\\nSetting 2 Given selected sections, predict an-\\nswers.\\nPrompt Y ou are a professional reviewer in key-\\nwords. Y ou will be given some sections extracted\\nfrom a paper in this domain. Based on the given\\ncontext, you need to answer the following multiple\\nchoice question. Y ou should select one or more\\nanswer choices from A, B, C, D.\\nB. Labeling Principle\\nB.1. Review aspect\\n• [Soundness] Questions related to the sound-\\nness of claims, supporting materials and\\nmathematical results.\\n• [Clarity] Questions of requesting more expla-\\nnations.\\n• [Comparison] Questions related to the com-\\nparison with related work: whether it is precise\\nand complete.\\n• [Substance] Questions to evaluate the num-\\nber of new ideas, results and the amount of\\nwork.\\n• [Citation] Specific questions of citations with-\\nout much comparison.\\n• [Reproducibility] Questions about code avail-\\nability , settings and hyperparameters.\\n• [Novelty] Questions to evaluate the signifi-\\ncance of problem, technique, methodology , or\\ninsight.\\n• [Format] Specific questions about the paper\\nformat.\\nB.2. Content aspect\\nQuestions related to different parts of the paper .\\nThe labels are: [Empirical Result], [Method], [Re-\\nlated Work], [Dataset], [Theoretical Result], [T ask],\\n[Abstract], [Evaluation], [PDF].\\nB.3. Ability\\nThe main ability needed to solve the question. If\\nmore than one ability is required, only choose the\\nmore complex one. The following labels are or-\\ndered increasingly by their complexity .\\n• [Knowledge] Questions about domain knowl-\\nedge, not the paper .\\n• [Summarize] Questions about general de-\\nscriptions of the paper . If it requires detailed\\ninformation or analysis of the reasoning pro-\\ncess, the use the [find] label.\\n• [Compare] Questions about comparing the\\npaper to other domain knowledge.\\n• [Find] Questions about detailed information\\nand logic.\\n• [Explain] Questions about further explana-\\ntions. If the explanation needs to add content,\\nfor example extra experiments or results, then\\nuse the [add] label.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='9351\\n• [Add] Questions about adding content to the\\noriginal paper . For example experiments, re-\\nsults, citations, etc. If the correction of old con-\\ntent is also involved, then use the [correct] la-\\nbel.\\n• [Correct] Questions about finding errors and\\nmake modifications.\\nB.4. Extra Information\\nOnly information from referenced papers (citations\\nin the paper or in the discussion forum) are consid-\\nered extra information.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94be5ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='LREC-COLING 2024, pages 9340–9351\\n20-25 May, 2024. © 2024 ELRA Language Resource Association: CC BY-NC 4.0\\n9340\\nIs LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM\\non Automatic Paper Reviewing T asks\\nRuiyang Zhou1 Lu Chen1,2, B Kai Yu1,2, B\\n1 X-LANCE Lab, Department of Computer Science and Engineering\\nMoE Key Lab of Artificial Intelligence, SJTU AI Institute\\nShanghai Jiao T ong University , Shanghai, China\\n2 Suzhou Laboratory , Suzhou, China\\n{ellenruiyang,chenlusz,kai.yu}@sjtu.edu.cn\\nAbstract\\nThe use of large language models (LLM), especially ChatGPT , to help with research has come into practice.\\nResearchers use it for timely advice and hope to obtain in-depth feedback. However , can LLM be a qualified\\nand reliable reviewer? Although there already exist several review-related datasets, few works have carefully and\\nthoroughly inspected model’s capability as a reviewer , especially the correctness of generated reviews. In this'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='thoroughly inspected model’s capability as a reviewer , especially the correctness of generated reviews. In this\\npaper , we first evaluate GPT -3.5 and GPT -4 (the current top-performing LLM) on 2 types of tasks under different\\nsettings: the score prediction task and the review generation task. In addition, we propose a dataset containing\\n196 review-revision multiple-choice questions (RR-MCQ) with detailed labels from the review-rebuttal forum in\\nICLR-2023. By asking questions from technical details to the overall presentation and quality , our RR-MCQ data\\nprovides a more complete model ability assessment. The results show that LLM is generally helpful, but great\\ncaution is needed as it always makes mistakes. Although it can give passable decisions (> 60% accuracy) on single\\noptions, completely correct answers are still rare (about 20%); models are still weak on long paper processing,\\nzero-shot scoring, and giving critical feedback like human reviewers.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='options, completely correct answers are still rare (about 20%); models are still weak on long paper processing,\\nzero-shot scoring, and giving critical feedback like human reviewers.\\nKeywords: automatic peer review, large language model, multiple choice question answering\\n1. Introduction\\nUtilizing large language models for scientific paper\\nreview recently attracts researcher’s interest. The\\ncontinuously growing amount of new paper pub-\\nlications, together with the increasing specializa-\\ntion within various research fields makes it a chal-\\nlenge to obtain timely and in-depth feedback. At\\nthe same time, LLM demonstrates strong ability\\nin reading comprehension, knowledge integration,\\nand even logical reasoning (OpenAI, 2023). Thus\\narises naturally this question: can LLM be a qual-\\nified and reliable automatic reviewer?\\nIn fact, even before the release of (truly) large lan-\\nguage models, there already exist datasets and\\nmethods targeting review-related tasks. For ex-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='ified and reliable automatic reviewer?\\nIn fact, even before the release of (truly) large lan-\\nguage models, there already exist datasets and\\nmethods targeting review-related tasks. For ex-\\nample, finetuning pretrained models to predict pa-\\nper decision and review scores (Li et al., 2020), or\\nusing language models to generate review texts\\n(Yuan et al. , 2022). Lately , new datasets for re-\\nview generation and edit generation also appear\\n(D’ Arcy et al., 2023), but there is still no detailed\\nassessment of model’s reviewing ability .\\nIn this paper , we first examine the reviewing abil-\\nity of GPT -3.5 and GPT -4 from two perspectives:\\nreview aspect score prediction and review genera-\\ntion. The two types of tasks evaluate both the abil-\\nity to discover flaws in research papers and rectify\\nthem, from the granularity level of abstract scor-\\ning to detailed commenting. We take great caution\\nB Lu Chen and Kai Yu are corresponding authors.\\nduring the evaluation process due to the innate dif-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='ing to detailed commenting. We take great caution\\nB Lu Chen and Kai Yu are corresponding authors.\\nduring the evaluation process due to the innate dif-\\nficulty of evaluating freely generated texts: besides\\nclassical automatic metrics, new metrics and man-\\nual evaluations are also implemented.\\nWe then design a “qualification exam” for fine-\\ngrained analysis: we construct 196 review-\\nrevision-related multiple-choice questions. On pre-\\nvious datasets like review generation ones, de-\\ntailed analyses are only possible when manually\\nexamining the generated text, resulting in huge\\ntime costs and subjective conclusions. Even with\\nmanual analysis, the correctness of generated re-\\nviews is still diﬀicult to measure. In contrast, our\\nRR-MCQ dataset with well-defined categorization\\nlabels enables comprehensive and satisfactory as-\\nsessments. The questions are inferred from real\\ndiscussion forums of 55 reviews from 14 papers in\\nICLR-2023, investigating both criticizing and cor-')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d891fde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='LREC-COLING 2024, pages 9340–9351\\n20-25 May, 2024. © 2024 ELRA Language Resource Association: CC BY-NC 4.0\\n9340\\nIs LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM\\non Automatic Paper Reviewing T asks\\nRuiyang Zhou1 Lu Chen1,2, B Kai Yu1,2, B\\n1 X-LANCE Lab, Department of Computer Science and Engineering\\nMoE Key Lab of Artificial Intelligence, SJTU AI Institute\\nShanghai Jiao T ong University , Shanghai, China\\n2 Suzhou Laboratory , Suzhou, China\\n{ellenruiyang,chenlusz,kai.yu}@sjtu.edu.cn\\nAbstract\\nThe use of large language models (LLM), especially ChatGPT , to help with research has come into practice.\\nResearchers use it for timely advice and hope to obtain in-depth feedback. However , can LLM be a qualified\\nand reliable reviewer? Although there already exist several review-related datasets, few works have carefully and\\nthoroughly inspected model’s capability as a reviewer , especially the correctness of generated reviews. In this'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='thoroughly inspected model’s capability as a reviewer , especially the correctness of generated reviews. In this\\npaper , we first evaluate GPT -3.5 and GPT -4 (the current top-performing LLM) on 2 types of tasks under different\\nsettings: the score prediction task and the review generation task. In addition, we propose a dataset containing\\n196 review-revision multiple-choice questions (RR-MCQ) with detailed labels from the review-rebuttal forum in\\nICLR-2023. By asking questions from technical details to the overall presentation and quality , our RR-MCQ data\\nprovides a more complete model ability assessment. The results show that LLM is generally helpful, but great\\ncaution is needed as it always makes mistakes. Although it can give passable decisions (> 60% accuracy) on single\\noptions, completely correct answers are still rare (about 20%); models are still weak on long paper processing,\\nzero-shot scoring, and giving critical feedback like human reviewers.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='options, completely correct answers are still rare (about 20%); models are still weak on long paper processing,\\nzero-shot scoring, and giving critical feedback like human reviewers.\\nKeywords: automatic peer review, large language model, multiple choice question answering\\n1. Introduction\\nUtilizing large language models for scientific paper\\nreview recently attracts researcher’s interest. The\\ncontinuously growing amount of new paper pub-\\nlications, together with the increasing specializa-\\ntion within various research fields makes it a chal-\\nlenge to obtain timely and in-depth feedback. At\\nthe same time, LLM demonstrates strong ability\\nin reading comprehension, knowledge integration,\\nand even logical reasoning (OpenAI, 2023). Thus\\narises naturally this question: can LLM be a qual-\\nified and reliable automatic reviewer?\\nIn fact, even before the release of (truly) large lan-\\nguage models, there already exist datasets and\\nmethods targeting review-related tasks. For ex-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='ified and reliable automatic reviewer?\\nIn fact, even before the release of (truly) large lan-\\nguage models, there already exist datasets and\\nmethods targeting review-related tasks. For ex-\\nample, finetuning pretrained models to predict pa-\\nper decision and review scores (Li et al., 2020), or\\nusing language models to generate review texts\\n(Yuan et al. , 2022). Lately , new datasets for re-\\nview generation and edit generation also appear\\n(D’ Arcy et al., 2023), but there is still no detailed\\nassessment of model’s reviewing ability .\\nIn this paper , we first examine the reviewing abil-\\nity of GPT -3.5 and GPT -4 from two perspectives:\\nreview aspect score prediction and review genera-\\ntion. The two types of tasks evaluate both the abil-\\nity to discover flaws in research papers and rectify\\nthem, from the granularity level of abstract scor-\\ning to detailed commenting. We take great caution\\nB Lu Chen and Kai Yu are corresponding authors.\\nduring the evaluation process due to the innate dif-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='ing to detailed commenting. We take great caution\\nB Lu Chen and Kai Yu are corresponding authors.\\nduring the evaluation process due to the innate dif-\\nficulty of evaluating freely generated texts: besides\\nclassical automatic metrics, new metrics and man-\\nual evaluations are also implemented.\\nWe then design a “qualification exam” for fine-\\ngrained analysis: we construct 196 review-\\nrevision-related multiple-choice questions. On pre-\\nvious datasets like review generation ones, de-\\ntailed analyses are only possible when manually\\nexamining the generated text, resulting in huge\\ntime costs and subjective conclusions. Even with\\nmanual analysis, the correctness of generated re-\\nviews is still diﬀicult to measure. In contrast, our\\nRR-MCQ dataset with well-defined categorization\\nlabels enables comprehensive and satisfactory as-\\nsessments. The questions are inferred from real\\ndiscussion forums of 55 reviews from 14 papers in\\nICLR-2023, investigating both criticizing and cor-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='labels enables comprehensive and satisfactory as-\\nsessments. The questions are inferred from real\\ndiscussion forums of 55 reviews from 14 papers in\\nICLR-2023, investigating both criticizing and cor-\\nrecting abilities. Due to the high cost of designing\\nhigh-quality questions, we limit the total number of\\nquestions to about 200 (196 to be specific).1\\nWe come to the following conclusions:\\n• LLM has the potential to give meaningful\\nscores and decide on individual statements.\\n• However , they are NOT easy to use in prac-\\ntice: seldom fully correct, not critical enough,\\n1 Our RR-MCQ data is available at https:\\n//huggingface.co/datasets/zhouruiyang/\\nRR-MCQ'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='9341\\nlack technical details, and struggle with long\\ncontext.\\n• Automatic similarity metrics do not align with\\nthe true review generation quality; the assess-\\nment of model reliability is needed.\\n2. Related Work\\n2.1. Paper-reviewing Related T ask\\nGeneration task Automatic review generation is\\nthe most direct task in using models as automatic\\nreviewers. Datasets like PeerRead ( Kang et al. ,\\n2018), ASAP ( Yuan et al. , 2022), ReviewRobot\\n(Wang et al. , 2020), MOPRD ( Lin et al. , 2023),\\nand NLPEER (Dycke et al., 2022) all contain scien-\\ntific papers (mostly in the domain of computer sci-\\nence) and their corresponding peer reviews. How-\\never , since it is diﬀicult to directly generate review\\ntexts and evaluate them, various types of annota-\\ntions have been proposed. The most common la-\\nbel is the sentence type, classified based on the\\nsentence’s sentiment polarity , review aspect, or\\nthe text aspect that it comments on, for example\\nCOMP ARE (Singh et al., 2021), ReAct (Choudhary'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='bel is the sentence type, classified based on the\\nsentence’s sentiment polarity , review aspect, or\\nthe text aspect that it comments on, for example\\nCOMP ARE (Singh et al., 2021), ReAct (Choudhary\\net al. , 2021), AMSR ( Fromm et al. , 2021), COM-\\nP ARE (Singh et al. , 2021), Peer-Review-Analyze\\n(Ghosal et al. , 2022), and AMPERE ( Hua et al. ,\\n2019). Still, there is no commonly recognized best\\nannotation style and evaluation metric.\\nOther generation tasks related to reviewing ability\\nalso appear , like meta-review generation and edit\\n(revision) generation. The meta-review generation\\ntask is more similar to the summarization task, but\\nit summarizes multiple peer reviews in the scien-\\ntific field. MreD ( Shen et al. , 2021) is an exempli-\\nfied meta-review generation task dataset with sen-\\ntence intent annotations. The revision generation\\ntask is more complicated, requiring the ability to\\ncomprehend the comment as well as take actions;\\nexamples are Revise and Resubmit ( Kuznetsov'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='tence intent annotations. The revision generation\\ntask is more complicated, requiring the ability to\\ncomprehend the comment as well as take actions;\\nexamples are Revise and Resubmit ( Kuznetsov\\net al. , 2022), ArxivEdits ( Jiang et al. , 2022) and\\nARIES (D’ Arcy et al., 2023).\\nClassification task Besides directly generating\\ntexts, more specific tasks with clear-cut answers\\nare actually more investigated. Paper decision pre-\\ndiction and aspect score prediction are the two\\nmost researched tasks before the appearance of\\nlarge language models, like in PeerRead ( Kang\\net al., 2018). Other reviewing-related tasks include\\nargument extraction in RR (Cheng et al., 2020) and\\nDISAPERE (Kennard et al., 2021), sentence clas-\\nsification (on datasets mentioned above for review\\ngeneration with annotations).\\nIn this paper , we first evaluate models on both\\ntypes of existing tasks: the generation task and\\nthe classification task. Specifically , we choose to'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='generation with annotations).\\nIn this paper , we first evaluate models on both\\ntypes of existing tasks: the generation task and\\nthe classification task. Specifically , we choose to\\nrun GPT -3.5 and GPT -4 on the review generation\\ntask and aspect score prediction task; the result of\\nGPT -4 on edit generation is already presented in\\nARIES (D’ Arcy et al., 2023). We then present our\\nRR-MCQ data that inspects all aspects mentioned\\nabove.\\n2.2. Large Language Models for\\nReviewing\\nRecently ,Liu and Shah (2023) inspects GPT -4’s\\nability by constructing a small-sized artificial test\\ndataset. They first create 13 short papers and\\nthen design test examples based on these brand-\\nnew papers to avoid the data leakage problem.\\nThey find that GPT -4 can accomplish the list-\\nchecking task, but makes frequent mistakes on\\nerror-identifying and paper-ranking tasks. Their\\ndetailed analysis is only based on a limited number\\nof manually designed questions; in contrast, our'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='checking task, but makes frequent mistakes on\\nerror-identifying and paper-ranking tasks. Their\\ndetailed analysis is only based on a limited number\\nof manually designed questions; in contrast, our\\nRR-MCQ dataset has more test questions whose\\ndistribution basically aligns with reality .\\nARIES ( D’ Arcy et al., 2023) proposes a dataset\\nfor comment-edit pairing and edit generation task,\\nbut find that even GPT -4 aligns badly the com-\\nment and the edit, and that the GPT -4 gener-\\nated revisions have low coherence and insuﬀi-\\ncient technical details. However , they do not mea-\\nsure the correctness of generated revisions as\\nit is extremely diﬀicult. Our work turns the free-\\ngeneration task into a multiple-choice question-\\nanswering task, making the measurement of cor-\\nrectness easy and automatic. It is like a qualifi-\\ncation test for LLM before being an automatic re-\\nviewer .\\nRobertson (2023) tests the usefulness of GPT -4\\ngenerated reviews by questioning 10 real users.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='cation test for LLM before being an automatic re-\\nviewer .\\nRobertson (2023) tests the usefulness of GPT -4\\ngenerated reviews by questioning 10 real users.\\nVery recently , Liang et al. (2023) assess on a\\nlarger scale the GPT -4 generated reviews: they\\ntag the comment overlap (hit rate and several other\\noverlap coeﬀicients) and survey the user satisfac-\\ntion to measure the review quality . They find that\\nthe reviews have satisfactory overlap and consis-\\ntency with human references, but can be non-\\ngeneric and emphasize different aspects. Com-\\nment overlap is an important indicator , but our RR-\\nMCQ data offers more evaluation perspectivesof\\nthe model’s ability and reliability .\\n3. T ask 1: Aspect Score Prediction\\n3.1. PeerRead Dataset\\nFor the task of aspect score prediction, we use the\\nICLR-2017 subset of the PeerRead dataset (Kang\\net al., 2018). This subset contains 1.3k manually\\nannotated aspect scores (ranging from 1 to 5 in-\\nclusive) for 427 oﬀicial reviews from ICLR-2017'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='ICLR-2017 subset of the PeerRead dataset (Kang\\net al., 2018). This subset contains 1.3k manually\\nannotated aspect scores (ranging from 1 to 5 in-\\nclusive) for 427 oﬀicial reviews from ICLR-2017\\nconference. The manual annotations ensure the\\nfeasibility and consistency of the aspect score pre-\\ndiction task: aspects that are not discussed in the\\nreview have a special not discussed score label.\\nSee Figure 1 for a concrete example.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='9342\\n1. accuracy↑ 2. |diff|↓ 3. Pearson↑ 4. Spearman↑ 5. Kendall’s tau↑\\nbaseline 1. most frequent score 0.404 0.966 0.333 0.340 0.297\\ngiven review\\n2. zero-shot 0.353 0.856 0.548 0.553 0.475\\n3. few-shot 0.306 1.132 0.651 0.659 0.580\\n4. MCQ style 0.336 1.025 0.558 0.565 0.492\\ngiven paper\\n5. abstract 0.237 0.992 0.228 0.233 0.195\\n6. whole paper (GPT -3.5-16k)0.138 2.132 0.131 0.131 0.109\\n7. selected sections 0.251 0.886 0.258 0.265 0.222\\n8. abstract & sections 0.330 0.923 0.248 0.249 0.209\\nT able 1: Average results of the aspect score prediction task from GPT -3.5 and GPT -3.5-16k on PeerRead\\ndataset. ↑ means the higher the metric value, the better the performance. The best result under each\\nsetting is bolded, and the best score across all settings is further italicized.\\n1. Recommendation2. Substance 3. Appropriateness 4. ComparisonP↑ Sp↑ K↑ P↑ Sp↑ K↑ P↑ Sp↑ K↑ P↑ Sp↑ K↑'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='setting is bolded, and the best score across all settings is further italicized.\\n1. Recommendation2. Substance 3. Appropriateness 4. ComparisonP↑ Sp↑ K↑ P↑ Sp↑ K↑ P↑ Sp↑ K↑ P↑ Sp↑ K↑\\n1. zero-shot 0.826 0.836 0.7570.394 0.414 0.3670.473 0.489 0.4390.393 0.399 0.3492. few-shot 0.807 0.811 0.7330.453 0.452 0.4130.634 0.604 0.5580.405 0.401 0.353given review 3. MCQ style 0.819 0.824 0.7440.430 0.432 0.3820.393 0.453 0.3920.344 0.313 0.273\\n4. abstract 0.283 0.282 0.25 0.187 0.190 0.1660.187 0.152 0.129 -0.023-0.031-0.0285. whole paper (GPT -3.5-16k)0.090 0.091 0.0800.000-0.003-0.003-0.100-0.019-0.097-0.0300.030 -0.0256. selected sections-0.076-0.081-0.0720.155 0.131 0.1170.202 0.222 0.197 0.021 0.011 0.009given paper\\n7. abstract & sections-0.014-0.008-0.0070.0320.051 0.046 0.002 -0.007-0.0070.080 0.076 0.068\\n5. Soundness 6. Originality 7. Clarity 8. ImpactP↑ Sp↑ K↑ P↑ Sp↑ K↑ P↑ Sp↑ K↑ P↑ Sp↑ K↑'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='7. abstract & sections-0.014-0.008-0.0070.0320.051 0.046 0.002 -0.007-0.0070.080 0.076 0.068\\n5. Soundness 6. Originality 7. Clarity 8. ImpactP↑ Sp↑ K↑ P↑ Sp↑ K↑ P↑ Sp↑ K↑ P↑ Sp↑ K↑\\n1. zero-shot 0.585 0.619 0.5420.507 0.510 0.4430.626 0.649 0.5720.445 0.450 0.3892. few-shot 0.667 0.674 0.6100.612 0.621 0.5450.730 0.745 0.6760.504 0.521 0.460given review 3. MCQ style 0.398 0.395 0.3550.476 0.469 0.4090.726 0.718 0.6440.494 0.493 0.436\\n4. abstract 0.120 0.117 0.1040.118 0.119 0.0980.172 0.171 0.1510.109 0.113 0.0975. whole paper (GPT -3.5-16k)0.052 0.0640.0570.0950.103 0.085-0.082-0.091-0.0810.069 0.086 0.0716. selected sections0.008 0.016 0.0140.197 0.189 0.1570.081 0.105 0.093 0.088 0.075 0.066given paper\\n7. abstract & sections0.157 0.183 0.1620.0840.079 0.067 0.112 0.1430.128 0.118 0.109 0.097\\nT able 2: Detailed aspect prediction results of GPT -3.5 and GPT -3.5-16k on PeerRead dataset. Numbers'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content=\"7. abstract & sections0.157 0.183 0.1620.0840.079 0.067 0.112 0.1430.128 0.118 0.109 0.097\\nT able 2: Detailed aspect prediction results of GPT -3.5 and GPT -3.5-16k on PeerRead dataset. Numbers\\nin gray color are values with p-value larger than 0.05. P is short for Pearson correlation, Sp for Spearman\\ncorrelation, and K for Kendall’s tau.\\nThe exposition is OK, and I \\nthink the approach is sensible, \\nbut the main issue with this \\npaper is that it is lacking \\nexperiments on non-synthetic \\ndatasets. As such, while I find \\nthe graphics inspired ques-\\ntions the paper is investigating \\ninteresting, I don't think it is \\nclear that this work introduces \\nuseful machinery for modeling \\nmore general videos. I think \\nthis paper is more appropriate \\nas a workshop contribution in \\nits current form.\\nTitle: Perception Updating \\nNetworks: On Architectural \\nConstraints For Interpretable \\nVideo Generative Models\\nAbstract: We investigate a \\nneural network architecture and \\nstatistical framework that...\"),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='Networks: On Architectural \\nConstraints For Interpretable \\nVideo Generative Models\\nAbstract: We investigate a \\nneural network architecture and \\nstatistical framework that...\\nSection 1 Introduction: The \\ncurrent computer graphics \\npipelines... Section 3 \\nPerception Updating Networks: \\nThis Section proposes a family \\nof neural architectures for \\noptimizing...\\nSetting 2: Given Paper\\nAspect Score\\nSetting 1: Given Review\\nRecommendation = 2\\nSubstance = 2\\nAppropriateness = 2\\nComparison = not discussed\\nSoundness = 3 \\nOriginality = not discussed\\nClarity = not discussed\\nImpact = 3 \\nLLM\\nFigure 1: Example of the aspect score prediction\\ntask. We conduct experiments under two settings:\\ngiven the review or the paper to predict scores.\\nWe conduct experiments under two different set-\\ntings: (1) given human-written review, predict as-\\npect scores; (2) given (part of) the research paper ,\\naccuracy↑ |diff|↓ Pearson↑ Spearman↑ Kendall’s tau↑'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='tings: (1) given human-written review, predict as-\\npect scores; (2) given (part of) the research paper ,\\naccuracy↑ |diff|↓ Pearson↑ Spearman↑ Kendall’s tau↑\\nmost freq0.317 0.813 0.628 0.630 0.560all 1 0.314 0.952 0.522 0.525 0.459all 5 0.337 0.822 0.546 0.539 0.476\\nT able 3: Average results of “only given abstract”\\nmethod on 100 randomly chosen examples from\\nPeerRead dataset. “most freq” means using the\\nmost frequent reference score for each aspect in\\nthe prompt example; similarly , “all 1” and “all 5”\\nmean setting all scores in the prompt example to 1\\nand 5.\\npredict scores.\\n3.2. Setting 1: Given Review, Predict\\nScores\\nT ests under the Setting 1 can be viewed as a re-\\nview reading comprehension task, targeting ques-\\ntions: are model-generated scores meaningful?\\nAre they consistent with the text? Can models\\nunderstand human-written reviews? In addition,\\nwe take into consideration the influence of prompt\\nstyle and content extraction methods. For Set-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='Are they consistent with the text? Can models\\nunderstand human-written reviews? In addition,\\nwe take into consideration the influence of prompt\\nstyle and content extraction methods. For Set-\\nting 1, we try zero-shot / few-shot, direct scoring\\n/ multiple-choice style scoring, and different exam-\\nple distributions.\\nBesides classical metrics accuracy and absolute'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='9343\\ndifference for the score prediction task, we also cal-\\nculate the correlation indicators of Pearson, Spear-\\nman, and Kendall’s tau. The three correlation met-\\nrics are the common choice when evaluating the\\nscore prediction ability of unfinetuned models, as\\nin the work of using LLM to evaluate abstractive\\nsummaries (Shen et al., 2023) and machine trans-\\nlations (Kocmi and Federmann, 2023).\\nIf not specially marked, all models are of version\\n0613 with temperature 0.3, for example GPT -3.5-\\nturbo-0613 in this section.\\nLLM can infer scores from reviews. As shown\\nin T able 1 (column 3 Pearson), when predicting\\nscores given the review, GPT -3.5 achieves a good\\ncorrelation with humans (0.651 Pearson correla-\\ntion value under the few-shot setting, while the\\nbaseline is only about 0.3). Even under the most\\ndiﬀicult zero-shot setting (line 2 zero-shot & col-\\numn 345), its correlations are still satisfactory\\n(above 0.5). This indicates that GPT -3.5 can un-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='diﬀicult zero-shot setting (line 2 zero-shot & col-\\numn 345), its correlations are still satisfactory\\n(above 0.5). This indicates that GPT -3.5 can un-\\nderstand human-written reviews, distinguish emo-\\ntions, and give consistent scores.\\nAnother result worth noticing is that the multiple-\\nchoice question style prompting does not help\\nmuch (line 4 MCQ style). In the MCQ style prompt,\\nwe write specific scoring criteria for each score in\\neach aspect but only obtain a small performance\\ngain compared to zero-shot prompting. We may\\nconclude that GPT -3.5 already knows the rules\\nand can inherently give meaningful scores.\\n3.3. Setting 2: Given Paper, Predict\\nScores\\nExperiments under the Setting 2 really assess\\nmodel’s capability to be a reviewer , answering our\\nmain research question. Under this task setting,\\nwe try three types of input for LLM: only abstract,\\nselected sections, and the whole paper (for GPT -\\n3.5-16k).\\nLLM fails to predict scores directly from pa-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='we try three types of input for LLM: only abstract,\\nselected sections, and the whole paper (for GPT -\\n3.5-16k).\\nLLM fails to predict scores directly from pa-\\npers. Unlike predicting scores given the review,\\nwhen only given (part of) the research paper , GPT -\\n3.5 struggles to generate reasonable scores. The\\nbottom half (line 5678) of T able1 shows that GPT -\\n3.5 only has 0.258 best Pearson correlation, even\\nlower than the baseline. The particularly poor per-\\nformance of GPT -3.5-16k (line 6) with correlations\\nlower than 0.2 gives us another indication: simply\\ninjecting long texts is not the way out, especially\\ncomplex long texts like research papers.\\nLLM predicts well the final [recommendation]\\nscore, but not scores of [comparison], [im-\\npact], and [substance] that are knowledge-\\nand logic-demanding. GPT -3.5 gains espe-\\ncially high correlations in predicting [Recommen-\\ndation] scores under both settings (Pearson corre-\\nlation 0.826 & 0.283, see detailed results for each'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='and logic-demanding. GPT -3.5 gains espe-\\ncially high correlations in predicting [Recommen-\\ndation] scores under both settings (Pearson corre-\\nlation 0.826 & 0.283, see detailed results for each\\naspect in T able2 column 1). However , it struggles\\nin judging [Comparison] (column 4) whether the pa-\\nper presents enough meaningful comparisons with\\nrelated work, [Substance] (column 2) whether it\\ncontains lots of ideas and results, and [Impact] (col-\\numn 8) whether it is influential and helpful to this\\nfield. We may attribute the diﬀiculty to the need of\\nextra scientific knowledge and details, as all three\\naspects require a rich understanding of the field.\\nHowever , we cannot exclude the possibility of us-\\ning memorized data to successfully predict the\\n[Recommendation] score (data leakage), as this\\nscore is the easiest to infer from other factors\\nand the PeerRead dataset uses ICLR-2017 pa-\\npers. Therefore, we present a more detailed exam-\\nination of model’s ability in Section 5 on our MCQ'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='score is the easiest to infer from other factors\\nand the PeerRead dataset uses ICLR-2017 pa-\\npers. Therefore, we present a more detailed exam-\\nination of model’s ability in Section 5 on our MCQ\\ntest data.\\nWe justify the choice of prompt example in T able3.\\nUsing the most frequent score of each aspect in\\nthe prompt has the best result, but the influence\\nis not decisive, as the variance among the three\\nprompts’ results is small. Therefore, we use the\\n“most frequent” score in the prompt example for all\\nexperiments in this section.\\n4. T ask 2: Review Generation\\n4.1. ASAP dataset\\nFor the task of review generation, we use the ICLR-\\n2020 subset of the ASAP dataset ( Yuan et al. ,\\n2022). Review sentences in this subset are la-\\nbeled by their aspect: summary , motivation, origi-\\nnality , soundness, substance, replicability , mean-\\ningful comparison, clarity ; each is further classi-\\nfied into positive and negative, except summary.\\nWe randomly select 300 papers from this subset'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content=\"ingful comparison, clarity ; each is further classi-\\nfied into positive and negative, except summary.\\nWe randomly select 300 papers from this subset\\nwith 902 corresponding oﬀicial peer reviews to test\\nmodel’s review generation ability . An example of\\nGPT -4 generated review is shown in Figure2.\\n[SUBSTANCE_POSITIVE] The authors' approach \\nto approximating the IBLagrangian through \\na second order Taylor expansion is innova-\\ntive. [ORIGINALITY_POSITIVE] The deriva-\\ntion of a lower bound that corresponds to \\nan PAC-Bayes prior is an original contri-\\nbution to this field. [SOUNDNESS_POSITIVE] \\nThe soundness of this study is demonstrat-\\ned through a number of experiments which \\nimply easy and hard cases where one can or \\ncannot prove generalization. [CLARITY_POS-\\nITIVE] The paper is well-structured and \\nclear, making complex concepts accessible \\nto readers. \\n[SUBSTANCE+]\\n[ORIGINALITY+]\\n[SOUNDNESS+]\\n[CLARITY+]\\nFigure 2: Example review generated by GPT -4.\"),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='ITIVE] The paper is well-structured and \\nclear, making complex concepts accessible \\nto readers. \\n[SUBSTANCE+]\\n[ORIGINALITY+]\\n[SOUNDNESS+]\\n[CLARITY+]\\nFigure 2: Example review generated by GPT -4.\\nThe sentence aspect label is part of the generation\\nand is put at the beginning of each sentence. A\\nspecial [None] label is added if the sentence does\\nnot belong to any other types.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='9344\\n4.2. Experiments and Results\\nAs in the aspect score prediction test in Section 3,\\nwe try zero-shot / few-shot prompting and different\\ncontent extraction methods in the two-step genera-\\ntion experiment. We present both the performance\\nof GPT -3.5-turbo-0613 and GPT -4-0613 for the re-\\nview generation task. Besides, we focus more on\\nthe evaluation method for model-generated review\\ntexts.\\nWe examine the quality from the following perspec-\\ntives. (1) Aspect coverage, thanks to the aspect\\nannotations in ASAP dataset. (2) Similarity to\\nreference reviews, including the classical statisti-\\ncal methods ROUGE-1/2/L (Lin, 2004), the model-\\nbased method BertScore ( Zhang et al. , 2019),\\nthe task-based reference-free metric BLANC (Vasi-\\nlyev et al. , 2020), and using GPT -4 to score the\\nsimilarity as in the evaluation of abstractive sum-\\nmaries (Shen et al., 2023) and of translation quality\\n(Kocmi and Federmann, 2023). (3) Manual analy-\\nsis for similarity and informativeness (whether con-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='maries (Shen et al., 2023) and of translation quality\\n(Kocmi and Federmann, 2023). (3) Manual analy-\\nsis for similarity and informativeness (whether con-\\ntaining enough details) proposed by Yuan and Liu\\n(2022) in KID-Review.\\nHere are some implementation details. GPT -3.5\\nis used for 300 randomly selected papers from\\nASAP , whose results are auto-evaluated in Fig-\\nure 3 and T able 5; GPT -4 is used for 50 papers\\nand the results are manually examined in T able6.\\nThe reason for only choosing 50 papers for GPT -\\n4 is that, the generation is expensive and that the\\nmanual analysis has also a high cost. For the two-\\nstep generation experiment, the section extraction\\nstep outputs the union of contents that are selected\\nbased on each review (most papers have three\\nreference reviews), and the union of useful con-\\ntents is input into the model to generate one re-\\nview. During the evaluation, the best similarity\\nscore is selected among all references for ROUGE'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='reference reviews), and the union of useful con-\\ntents is input into the model to generate one re-\\nview. During the evaluation, the best similarity\\nscore is selected among all references for ROUGE\\nand BertScore; for BLANC, all reference reviews\\nare concatenated to form the new reference.\\nLLM has its own comment aspect preference:\\ntoo much positive feedback. In Figure 3, it is\\nclear that GPT -3.5 has its own preference of as-\\npects to comment on: it always generates positive\\nreviews while being very cautious about negative\\naspects. For each setting, the proportion of posi-\\ntive feedback (exclude [Summary]) is always larger\\nthan 55% (reference labels are only 43% positive).\\nEspecially when only given the abstract, 99% com-\\nments are positive. In addition, [Substance-] and\\n[Clarity-] are always missing in all four experiment\\nsettings.\\nFew-shot prompting helps GPT -3.5 to generate\\nmore negative comments, but the distribution is\\nstill very different from reality . We regard this as'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='settings.\\nFew-shot prompting helps GPT -3.5 to generate\\nmore negative comments, but the distribution is\\nstill very different from reality . We regard this as\\na strong weakness for GPT -3.5 being a reviewer\\nbecause well-supported critics are the most help-\\nful for researchers.\\nFigure 3: Aspect label distribution of reference\\nreviews and model-generated reviews on ASAP\\ndataset. “+ / -” means positive/ negative com-\\nments, except that [summary] is always neutral.\\nReference label distribution comes from the union\\nof all reviews for each paper .\\nThe aspect recall value in T able 5 (column 1 as-\\npect) also indicates that both GPT -3.5 and GPT -4\\nhave very different comment preferences from hu-\\nmans. Although their best aspect coverage rate\\n(0.582) is better than humans (0.499), the highest\\naspect recall is still unsatisfactory (0.559). Even\\nunder the few-shot prompt setting (line 2 few-shot),\\nthis value is still low (0.506), showing that LLM can-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='aspect recall is still unsatisfactory (0.559). Even\\nunder the few-shot prompt setting (line 2 few-shot),\\nthis value is still low (0.506), showing that LLM can-\\nnot naturally generate comments of people’s inter-\\nest.\\nAutomatic metrics can be fooled by LLM’s gen-\\neration. In T able4, to better examine the review\\ngeneration quality , we try using GPT -4 as an eval-\\nuator to predict the relevance, precision, and re-\\ncall score (between 0 and 100). We also manu-\\nally score (also between 0 and 100) the 50 reviews\\ngenerated by GPT -4-0613 when given both the ab-\\nstract and selected sections. We focus on two per-\\nspectives for quality evaluation: (1) relevance to\\nreference reviews; (2) amount of details in the re-\\nview (informativeness).\\nDuring the process of manual inspection, we find\\nthat GPT -4 generated reviews are strongly influ-\\nenced by the given paper content and lack crit-\\nics, while reference reviews contain lots of detailed\\nsuggestions. This is why the manual score is low,'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='enced by the given paper content and lack crit-\\nics, while reference reviews contain lots of detailed\\nsuggestions. This is why the manual score is low,\\n54.5 for relevance and 48.7 for informativeness.\\nHowever , the manual score (and also our actual im-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='9345\\nGPT -4 predictedrelevance↑ precision↑ recall↑\\n83.8 84.7 75.8\\nmanually scoredrelevance↑ informativeness↑\\n54.5 48.7\\nT able 4: “GPT -4 as review quality evaluator” pre-\\ndicted scores and manually annotated scores for\\nthe 50 GPT -4 generated reviews.\\npression) is very different from BertScore (always\\nabove 0.8): is BertScore fooled by the fluent gener-\\nation? Thus, we compute the correlation between\\nevery automatically computed similarity score and\\nour manual annotations in T able 6. As expected,\\nBertScore has the worst correlation (-0.003 and -\\n0.024), showing that BertScore is fooled by GPT -4\\ngenerated texts.\\nAutomatic metrics that best correspond to humans\\nare BLANC and GPT -4. BLANC is designed for no-\\nreference summary evaluation, but here we con-\\nsider the generated review as a summary of all\\nreference reviews. BLANC uses the performance\\ngain on the blank-filling task as the indicator: if a\\nsummary can help its model to complete the origi-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='sider the generated review as a summary of all\\nreference reviews. BLANC uses the performance\\ngain on the blank-filling task as the indicator: if a\\nsummary can help its model to complete the origi-\\nnal task, then the summary is considered of good\\nquality . This metric truly tests the amount of infor-\\nmation contained in the summary and is diﬀicult to\\nbe fooled by fluent but hollow sentences.\\nIn conclusion, GPT -4 does not naturally provide\\nenough details and critics; automatic similarity met-\\nrics can be unaware of the flaws. Since it is still dif-\\nficult to evaluate model-generated reviews, we use\\nour RR-MCQ data in Section 5, a more objective\\nand detailed approach to evaluation.\\n5. T ask 3: Review-Revision\\nMultiple-Choice Questions\\nThe experiments of aspect score prediction and\\nreview generation above do give us an idea of\\nmodel’s reviewing ability . Moreover , GPT -4 has\\nalso been tested on the edit generation task in\\nARIES ( D’ Arcy et al. , 2023). They manually'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='review generation above do give us an idea of\\nmodel’s reviewing ability . Moreover , GPT -4 has\\nalso been tested on the edit generation task in\\nARIES ( D’ Arcy et al. , 2023). They manually\\nannotate 85 generated examples from the fol-\\nlowing perspectives: compliance (1-3), promise\\n(true/false), paraphrase (true/false), and technical\\ndetails (true/false). They find that GPT -4 has high\\ncompliance (94% are scored 3), but tends to make\\npromises (21%), simply paraphrase the comment\\n(48%), and lacks technical details (12%).\\nOne evident drawback of the previous analyses is\\nthat, all detailed results require heavy manual in-\\nspection while automatic metrics are not reliable\\nenough (see Section 4.2). Therefore, we propose\\na review-revision multiple-choice question dataset\\n(RR-MCQ) with abundant labels, containing ques-\\ntions related to both the review and revision with\\none or more correct answers.\\n5.1. RR-MCQ Data Construction and\\nCharacteristics\\nQuestion:'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='(RR-MCQ) with abundant labels, containing ques-\\ntions related to both the review and revision with\\none or more correct answers.\\n5.1. RR-MCQ Data Construction and\\nCharacteristics\\nQuestion:\\nWhat experiments could be added to illustrate \\nthe proposed method’s effectiveness?\\nA. change the similarity threshold\\nB. use the same examples for different models\\nC. use randomly sampled examples for \\ndifferent models\\nD. do not use the similarity threshold\\nOptions:\\nAnswers: A B\\nLabels: [soundness] [empirical] [add] [no-need]\\nFigure 4: Example of the multiple-choice question\\nwith one or more answers. We randomly shuffle\\nthe four options during the experiment.\\nOur RR-MCQ dataset is targeted for a more spe-\\ncific and in-depth assessment. For example, can\\nmodels evaluate the soundness of argumentation?\\nCan they integrate domain knowledge and the pa-\\nper together? Can they give complicated sugges-\\ntions, such as important experiments to do?\\nThe dataset contains 196 multiple-choice ques-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='Can they integrate domain knowledge and the pa-\\nper together? Can they give complicated sugges-\\ntions, such as important experiments to do?\\nThe dataset contains 196 multiple-choice ques-\\ntions examining specific review-revision-related\\nknowledge and ability . An example of the RR-\\nMCQ is presented in Figure 4.\\nT o construct the MCQ test dataset, we select 55\\nreviews from 14 papers with suﬀicient comment-\\nresponse posts in the peer review forum from the\\nICLR-2023 conference. We then perform the fol-\\nlowing four steps: (1) align the smallest unit of com-\\nment and response to form a single argument; (2)\\nidentify its main topic and decide if controversial\\n(we skip arguments on which the two sides sharply\\ndisagree); (3) transform the argument into a four-\\nchoice question without adding new contents, i.e.\\nwrong options either come from irrelevant parts of\\nthe same discussion or are the negation of correct\\nones; (4) label the aspects being assessed by the\\nquestion.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='wrong options either come from irrelevant parts of\\nthe same discussion or are the negation of correct\\nones; (4) label the aspects being assessed by the\\nquestion.\\nThere are four types of labels corresponding to\\nfour ways of categorization: review aspect, con-\\ntent aspect, ability to be tested, if need informa-\\ntion from other papers. Figure 5 presents the la-\\nbel distribution, which basically corresponds to the\\nreview comment distribution in reality . The labels\\nare assigned by two experienced students in the\\ndomain. Among all the 788 annotated labels, 86\\nlabels (10.9%) have disagreement at first. The fi-\\nnal decision is made through a careful discussion\\nof the two annotators.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='9346\\n1. aspect (macro avg) 2. ROUGE-F1 (macro avg)\\nref coverage↑ coverage↑ recall ↑ R-1 ↑ R-2 ↑ R-L ↑\\nabstract 1. zero-shot 0.499 0.034 0.065 0.429 0.103 0.190\\n2. few-shot 0.499 0.582 0.506 0.424 0.108 0.198\\npaper 3. selected sections 0.499 0.367 0.449 0.382 0.081 0.176\\n4. abstract & sections 0.499 0.470 0.559 0.431 0.106 0.193\\nGPT -4* 5. abstract & sections 0.488* 0.515* 0.530* 0.425* 0.100* 0.190*\\n3. BertScore (macro avg) 4. BLANC\\nprecision↑ recall ↑ F1 ↑ average↑ variance\\nabstract 1. zero-shot 0.855 0.835 0.843 0.098 0.001\\n2. few-shot 0.851 0.839 0.843 0.114 0.001\\npaper 3. selected sections 0.843 0.832 0.835 0.093 0.001\\n4. abstract & sections 0.845 0.840 0.841 0.126 0.001\\nGPT -4* 5. abstract & sections 0.851* 0.840* 0.843* 0.112* 0.001*\\nT able 5: Automatic evaluation of GPT -3.5 (300 examples) and GPT -4 (50 examples) generated reviews\\non ASAP dataset. GPT -4 results are noted with * because they are averaged over 50 examples.\\nrelevance informativeness\\naspect-recall 0.076 0.227'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='on ASAP dataset. GPT -4 results are noted with * because they are averaged over 50 examples.\\nrelevance informativeness\\naspect-recall 0.076 0.227\\nRouge1-F1 0.115 0.159\\nRouge2-F1 0.039 0.111\\nRougeL -F1 0.167 0.124\\nBertScore-F1 -0.003 -0.024\\nBLANC-avg 0.255 0.355\\nGPT4-avg 0.325 0.244\\nT able 6: Pearson correlation values between auto-\\nmatic evaluation metrics and manually annotated\\nreview quality labels for the 50 GPT -4 generated\\nreviews.\\nFigure 5: Label distribution of our RR-MCQ test\\ndata. There are 4 types of labels: review as-\\npect, content aspect, ability , and if need informa-\\ntion from other papers.\\n5.2. Experiments and Results\\nWe test both GPT -3.5-turbo-0613 and GPT -4-0613\\non our MCQ data. The two-step generation\\nmethod is similar to that of Section 4: the model\\nselects useful sections based on the given ques-\\ntion, then the selected contents are input into the\\nmodel to predict answers. Note that our multiple-\\nchoice questions may have more than one correct\\nanswer .'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='tion, then the selected contents are input into the\\nmodel to predict answers. Note that our multiple-\\nchoice questions may have more than one correct\\nanswer .\\nLLM gets passable micro accuracy, but bad\\nmacro accuracy. From T able7, the best micro\\naccuracy 0.710 comes from the GPT -4 -> GPT -4\\nmethod. It is a passable score, but the macro ac-\\ncuracy is not ideal: the best macro accuracy is only\\n0.276. The micro accuracy is calculated by consid-\\nering each question option as an individual exam-\\nple so that the MCQ task becomes a binary de-\\ncision task of determining T rue/False for each op-\\ntion. The macro accuracy is more strict: only when\\nall answers are correct, the question is marked as\\ncorrect (note that the number of correct options is\\nundetermined, between one and four).\\nIn addition, the precision and recall are balanced.\\nWe can conclude that GPT -4 is able to judge\\nthe correctness of each individual statement with\\nabout 70% accuracy , but this level of ability is in-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='We can conclude that GPT -4 is able to judge\\nthe correctness of each individual statement with\\nabout 70% accuracy , but this level of ability is in-\\nsuﬀicient for giving completely correct answers: er-\\nrors and omissions are frequent.\\nLLM struggles with tasks that relate to sound-\\nness and adding components. We select the\\ntop 2 numerous labels in each of the 4 label cate-\\ngories and present the detailed results in T able 8.\\nAs our questions are inferred from the true peer\\nreview discussion forum (containing both the re-\\nview and the response), these types of questions\\nare also the most common in reality . Therefore,\\nmodel’s performance on these questions is impor-\\ntant and representative.\\nIn T able 8, [Explain] related questions have the\\nhighest macro accuracy (by comparing the bold\\nnumber in “accuracy” through all eight aspects).\\nThese types of questions come from discussions\\nto confirm an understanding or ask a detail, some-\\nhow similar to reading comprehension and essay'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='number in “accuracy” through all eight aspects).\\nThese types of questions come from discussions\\nto confirm an understanding or ask a detail, some-\\nhow similar to reading comprehension and essay\\nexpansion tasks. It does not require much logi-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='9347\\nmacro average micro average\\naccuracy↑ precision↑ recall↑ F1 ↑ accuracy↑ precision↑ recall↑ F1 ↑\\nGPT -3.5 -> GPT -3.5 0.128 0.332 0.376 0.176 0.569 0.583 0.373 0.227\\nGPT -4 -> GPT -3.5 0.214 0.553 0.586 0.285 0.648 0.644 0.603 0.311\\nGPT -4 -> GPT -4 0.276 0.655 0.666 0.330 0.710 0.699 0.701 0.350\\nT able 7: Results on RR-MCQ test data. “GPT -4 -> GPT -3.5” means using GPT -4 for the first section\\nselection step and then using GPT -3.5 for the second answer generation step.\\n1. Soundness 2. Clarity 3. Empirical 4. Method\\nmacro average acc↑ prec↑ recall↑ acc↑ prec↑ recall↑ acc↑ prec↑ recall↑ acc↑ prec↑ recall↑\\nGPT -3.5 -> GPT -3.50.091 0.293 0.307 0.194 0.356 0.454 0.055 0.267 0.308 0.173 0.391 0.458\\nGPT -4 -> GPT -3.50.205 0.526 0.552 0.278 0.498 0.560 0.209 0.573 0.597 0.247 0.564 0.598\\nGPT -4 -> GPT -40.193 0.655 0.673 0.361 0.514 0.509 0.253 0.713 0.695 0.309 0.617 0.654\\n5. Explain 6. Add 7. No need 8. Need'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='GPT -4 -> GPT -40.193 0.655 0.673 0.361 0.514 0.509 0.253 0.713 0.695 0.309 0.617 0.654\\n5. Explain 6. Add 7. No need 8. Need\\nmacro average acc↑ prec↑ recall↑ acc↑ prec↑ recall↑ acc↑ prec↑ recall↑ acc↑ prec↑ recall↑\\nGPT -3.5 -> GPT -3.50.182 0.374 0.384 0.051 0.284 0.336 0.118 0.295 0.358 0.140 0.380 0.400\\nGPT -4 -> GPT -3.50.247 0.573 0.534 0.203 0.586 0.609 0.227 0.523 0.549 0.198 0.591 0.634\\nGPT -4 -> GPT -40.364 0.667 0.626 0.153 0.694 0.757 0.291 0.602 0.647 0.256 0.724 0.691\\nT able 8: Detailed results on RR-MCQ test data. For each label category , we present only the detailed\\nresults for the top two labels with the most elements.\\ncal reasoning, but lots of knowledge and inference\\nability .\\nQuestion [Soundness]:\\nWhich choice the author makes may hurt the \\nfinal performance but is not well-examined in \\nthe paper?\\nA. linearize the RNN module\\nB. use a learnable gate\\nC. combine RNN and self-attention\\nD. REM endows positional encodings of a \\nmulti-head self-attention with recurrent'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='the paper?\\nA. linearize the RNN module\\nB. use a learnable gate\\nC. combine RNN and self-attention\\nD. REM endows positional encodings of a \\nmulti-head self-attention with recurrent \\ndynamics\\nOptions:\\nAnswers: A\\nFigure 6: Example question of label [Soundness].\\nThe lowest macro accuracy appears in questions\\nof labels [Soundness] and [Add] (note that one\\nquestion may have both the [Soundness] and\\n[Add] labels because they mark different perspec-\\ntives).\\nAn example of [Soundness] question is shown in\\nFigure 6. The review aspect label [Soundness]\\nconcerns questions requiring strong logic, for ex-\\nample, the correctness of a statement, the validity\\nof an argument, or the completeness of supporting\\nevidence. In the example above, the model needs\\nto first identify whether the choice is influential, and\\nthen decide if it is well-examined in the paper .\\nAn example of [Add] question is shown in Figure7.\\nQuestion [Add]:\\nWhat possible experiments can be added to \\npresent the method’s usefulness ?'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='then decide if it is well-examined in the paper .\\nAn example of [Add] question is shown in Figure7.\\nQuestion [Add]:\\nWhat possible experiments can be added to \\npresent the method’s usefulness ?\\nA. compare to linear freezing and AutoFreeze\\nB. try DeiT-T model on ImageNet dataset\\nC. try Bert model on MRPC dataset\\nD. try Bert model on CoLA dataset\\nOptions:\\nAnswers: B C D\\nFigure 7: Example question of label [Add].\\nThe tested ability label [Add] relates to adding com-\\nponents to the paper , for example, conducting an\\nextra experiment or citing a missing related work.\\nIts diﬀiculty comes from the need for both logic\\nand knowledge. Although possible options to be\\nadded are already provided in the question, the\\nmodel still has to carefully select truly necessary\\nones. In the example question above, GPT -4 fails\\nto understand that the first option is already in the\\npaper and that the second option is needed to\\nprove method’s effectiveness on other model ar-\\nchitectures.\\n6. Conclusion'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='to understand that the first option is already in the\\npaper and that the second option is needed to\\nprove method’s effectiveness on other model ar-\\nchitectures.\\n6. Conclusion\\n“Can LLM be a qualified and reliable automatic re-\\nviewer?” After testing GPT -3.5 and GPT -4 on two\\nexisting datasets and also on our proposed RR-\\nMCQ data, we conclude that they are not natu-\\nrally reliable automatic reviewers because their er-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='9348\\nror rate is still not suﬀiciently low.\\nThey can generate meaningful scores based on\\nhuman-written reviews, even without explicitly giv-\\ning examples or scoring criteria; but when the in-\\nput is long and complicated like a whole research\\npaper , they can only roughly identify the quality .\\nWhen being asked to freely generate comments,\\ntheir suggestions are sometimes correct, but al-\\nways on aspects that human reviewers would not\\nbe interested in. Facing multiple-choice questions,\\nthey have the ability to make passable decisions\\non single options, but hardly to be completely cor-\\nrect .\\nWe claim that it is still too early to trust LLM as auto-\\nmatic scientific paper reviewer . Although there is a\\nchance to get useful and correct results, their cur-\\nrent capability is not reliable enough. Especially on\\nquestions requiring logic reasoning over long texts\\nor extra knowledge in detail, their performance is\\nstill unsatisfactory .\\nWe believe that in detail and in-depth evaluations'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='questions requiring logic reasoning over long texts\\nor extra knowledge in detail, their performance is\\nstill unsatisfactory .\\nWe believe that in detail and in-depth evaluations\\nare needed before the targeted development of\\nLLM in automatic paper reviewing task. Our RR-\\nMCQ test dataset is an example, but still with lim-\\nited size. Future work could be to develop bet-\\nter data construction methods, to invent new met-\\nrics, and finally to improve LLM’s capability as a\\nreviewer .\\n7. Acknowledgements\\nThis work was supported by the National Key\\nR&D Program of China 2023ZD0120703 and the\\nChina NSFC Projects (U23B2057, 62106142\\nand 62120106006) and Shanghai Munici-\\npal Science and T echnology Major Project\\n(2021SHZDZX0102).\\n8. Bibliographical References\\nLiying Cheng, Lidong Bing, Qian Yu, Wei Lu, and\\nLuo Si. 2020. Ape: Argument pair extrac-\\ntion from peer review and rebuttal via multi-task\\nlearning. In Proceedings of the 2020 Confer-\\nence on Empirical Methods in Natural Language'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='Luo Si. 2020. Ape: Argument pair extrac-\\ntion from peer review and rebuttal via multi-task\\nlearning. In Proceedings of the 2020 Confer-\\nence on Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 7000–7011.\\nGautam Choudhary , Natwar Modani, and Ni-\\ntish Maurya. 2021. React: A re view com-\\nment dataset for act ionability (and more). In\\nWeb Information Systems Engineering–WISE\\n2021: 22nd International Conference on Web\\nInformation Systems Engineering, WISE 2021,\\nMelbourne, VIC, Australia, October 26–29,\\n2021, Proceedings, Part II 22 , pages 336–343.\\nSpringer .\\nMike D’ Arcy , Alexis Ross, Erin Bransom, Bailey\\nKuehl, Jonathan Bragg, T om Hope, and Doug\\nDowney . 2023. Aries: A corpus of scientific\\npaper edits made in response to peer reviews.\\narXiv preprint arXiv:2306.12587.\\nNils Dycke, Ilia Kuznetsov , and Iryna Gurevych.\\n2022. Nlpeer: A unified resource for the com-\\nputational study of peer review. arXiv preprint\\narXiv:2211.06651.\\nMichael Fromm, Evgeniy Faerman, Max Berren-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='2022. Nlpeer: A unified resource for the com-\\nputational study of peer review. arXiv preprint\\narXiv:2211.06651.\\nMichael Fromm, Evgeniy Faerman, Max Berren-\\ndorf, Siddharth Bhargava, Ruoxia Qi, Y ao\\nZhang, Lukas Dennert, Sophia Selle, Y ang Mao,\\nand Thomas Seidl. 2021. Argument mining\\ndriven analysis of peer-reviews. In Proceedings\\nof the AAAI Conference on Artificial Intelligence,\\nvolume 35, pages 4758–4766.\\nTirthankar Ghosal, Sandeep Kumar , Prabhat Ku-\\nmar Bharti, and Asif Ekbal. 2022. Peer review\\nanalyze: A novel benchmark resource for com-\\nputational analysis of peer reviews. Plos one ,\\n17(1):e0259238.\\nXinyu Hua, Mitko Nikolov , Nikhil Badugu, and\\nLu Wang. 2019. Argument mining for un-\\nderstanding peer reviews. arXiv preprint\\narXiv:1903.10104.\\nChao Jiang, Wei Xu, and Samuel Stevens. 2022.\\narxivedits: Understanding the human revision\\nprocess in scientific writing. arXiv preprint\\narXiv:2210.15067.\\nDongyeop Kang, Waleed Ammar , Bhavana Dalvi,'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='Chao Jiang, Wei Xu, and Samuel Stevens. 2022.\\narxivedits: Understanding the human revision\\nprocess in scientific writing. arXiv preprint\\narXiv:2210.15067.\\nDongyeop Kang, Waleed Ammar , Bhavana Dalvi,\\nMadeleine Van Zuylen, Sebastian Kohlmeier ,\\nEduard Hovy , and Roy Schwartz. 2018. A\\ndataset of peer reviews (peerread): Collection,\\ninsights and nlp applications. arXiv preprint\\narXiv:1804.09635.\\nNeha Kennard, Tim O’Gorman, Rajarshi Das, Ak-\\nshay Sharma, Chhandak Bagchi, Matthew Clin-\\nton, Pranay Kumar Y elugam, Hamed Zamani,\\nand Andrew McCallum. 2021. Disapere: A\\ndataset for discourse structure in peer review\\ndiscussions. arXiv preprint arXiv:2110.08520.\\nT om Kocmi and Christian Federmann. 2023. Large\\nlanguage models are state-of-the-art evalua-\\ntors of translation quality . arXiv preprint\\narXiv:2302.14520.\\nIlia Kuznetsov , Jan Buchmann, Max Eichler , and\\nIryna Gurevych. 2022. Revise and resubmit:\\nAn intertextual model of text-based collabora-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='arXiv:2302.14520.\\nIlia Kuznetsov , Jan Buchmann, Max Eichler , and\\nIryna Gurevych. 2022. Revise and resubmit:\\nAn intertextual model of text-based collabora-\\ntion in peer review. Computational Linguistics,\\n48(4):949–986.\\nJiyi Li, Ayaka Sato, Kazuya Shimura, and Fumiyo\\nFukumoto. 2020. Multi-task peer-review score\\nprediction. In Proceedings of the First Workshop\\non Scholarly Document Processing, pages 121–\\n126.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='9349\\nWeixin Liang, Yuhui Zhang, Hancheng Cao,\\nBinglu Wang, Daisy Yi Ding, Xinyu Y ang, Kailas\\nVodrahalli, Siyu He, Daniel Scott Smith, Yian\\nYin, et al. 2023. Can large language models\\nprovide useful feedback on research papers?\\na large-scale empirical analysis. arXiv preprint\\narXiv:2310.01783.\\nChin- Y ew Lin. 2004. Rouge: A package for auto-\\nmatic evaluation of summaries. In T ext summa-\\nrization branches out, pages 74–81.\\nJialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong\\nChen, and Xiaodong Shi. 2023. Moprd: A mul-\\ntidisciplinary open peer review dataset. Neural\\nComputing and Applications, pages 1–16.\\nRyan Liu and Nihar B Shah. 2023. Reviewergpt?\\nan exploratory study on using large language\\nmodels for paper reviewing. arXiv preprint\\narXiv:2306.00622.\\nOpenAI. 2023. Gpt-4 technical report.\\nZachary Robertson. 2023. Gpt4 is slightly helpful\\nfor peer-review assistance: A pilot study . arXiv\\npreprint arXiv:2307.05492.\\nChenhui Shen, Liying Cheng, Y ang Y ou, and Li-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='Zachary Robertson. 2023. Gpt4 is slightly helpful\\nfor peer-review assistance: A pilot study . arXiv\\npreprint arXiv:2307.05492.\\nChenhui Shen, Liying Cheng, Y ang Y ou, and Li-\\ndong Bing. 2023. Are large language models\\ngood evaluators for abstractive summarization?\\narXiv preprint arXiv:2305.13091.\\nChenhui Shen, Liying Cheng, Ran Zhou, Li-\\ndong Bing, Y ang Y ou, and Luo Si. 2021.\\nMred: A meta-review dataset for structure-\\ncontrollable text generation. arXiv preprint\\narXiv:2110.07474.\\nShruti Singh, Mayank Singh, and Pawan Goyal.\\n2021. Compare: a taxonomy and dataset of\\ncomparison discussions in peer reviews. In\\n2021 ACM/IEEE Joint Conference on Digital Li-\\nbraries (JCDL), pages 238–241. IEEE.\\nOleg Vasilyev , Vedant Dharnidharka, and John\\nBohannon. 2020. Fill in the blanc: Human-\\nfree quality estimation of document summaries.\\narXiv preprint arXiv:2002.09836.\\nQingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight,\\nHeng Ji, and Nazneen Fatema Rajani. 2020. Re-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='free quality estimation of document summaries.\\narXiv preprint arXiv:2002.09836.\\nQingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight,\\nHeng Ji, and Nazneen Fatema Rajani. 2020. Re-\\nviewrobot: Explainable paper review generation\\nbased on knowledge synthesis. arXiv preprint\\narXiv:2010.06119.\\nWeizhe Yuan and Pengfei Liu. 2022. Kid-review:\\nKnowledge-guided scientific review generation\\nwith oracle pre-training. In Proceedings of the\\nAAAI Conference on Artificial Intelligence , vol-\\nume 36, pages 11639–11647.\\nWeizhe Yuan, Pengfei Liu, and Graham Neu-\\nbig. 2022. Can we automate scientific review-\\ning? Journal of Artificial Intelligence Research ,\\n75:171–212.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\\nWeinberger , and Y oav Artzi. 2019. Bertscore:\\nEvaluating text generation with bert. arXiv\\npreprint arXiv:1904.09675.\\nA. Prompt\\nA.1. Evaluation on PeerRead\\nSetting 1 Given review, predict scores.\\nPrompt Y ou are a professional reviewer in com-\\nputer science and machine learning. Based on'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='preprint arXiv:1904.09675.\\nA. Prompt\\nA.1. Evaluation on PeerRead\\nSetting 1 Given review, predict scores.\\nPrompt Y ou are a professional reviewer in com-\\nputer science and machine learning. Based on\\nthe given review, you need to predict the review\\nscore in several aspects. Choose a score from\\n[1,2,3,4,5], higher score means better paper qual-\\nity .\\nZero-Shot Example Example output: RECOM-\\nMENDA TION: x, SUBST ANCE: x, APPROPRI-\\nA TENESS: x, MEANINGFU COMP ARISON: x,\\nSOUNDNESS CORRECTNESS: x, ORIGINAL -\\nITY : x, CLARITY : x, IMP ACT : x\\nFew-Shot Example Example1: Review: This\\npaper presents an approach to modeling videos\\nbased on a decomposition into a background ...\\nworkshop contribution in its current form. Output:\\nRECOMMENDA TION: 2, SUBST ANCE: 2, AP-\\nPROPRIA TENESS: 2, SOUNDNESS CORRECT -\\nNESS: 3, IMP ACT : 3 . Example2 ... Example5 ...\\nMCQ-Style Example RECOMMENDA TION: A.\\nThis paper changed my thinking on this topic and\\nI’d fight to get it accepted; B. I learned a lot from this'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='NESS: 3, IMP ACT : 3 . Example2 ... Example5 ...\\nMCQ-Style Example RECOMMENDA TION: A.\\nThis paper changed my thinking on this topic and\\nI’d fight to get it accepted; B. I learned a lot from this\\npaper and would like to see it accepted. C. Border-\\nline: I am ambivalent about this one. D. Leaning\\nagainst: I would rather not see it in the conference.\\nE. Poor: I would fight to have it rejected.\\nSetting 2 Given paper , predict scores.\\nPrompt Y ou are a professional reviewer in com-\\nputer science and machine learning. Based on\\nthe given abstract/sections/paper , you need to pre-\\ndict the review score in several aspects. Choose a\\nscore from [1,2,3,4,5], higher score means better\\npaper quality .\\nA.2. Evaluation on ASAP\\nSetting 1 Given paper , generate review text.\\nPrompt Y ou are a professional reviewer in com-\\nputer science and machine learning. Based\\non the given title and abstract of a research\\npaper , you need to write a review in ICLR\\nstyle. At the same time, you need to tag'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='puter science and machine learning. Based\\non the given title and abstract of a research\\npaper , you need to write a review in ICLR\\nstyle. At the same time, you need to tag\\nsequences of words with their review type'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='9350\\nat the beginning: [NONE], [SUMMARY], [MO-\\nTIV A TION POSITIVE], [[MOTIV A TION NEGA-\\nTIVE]], [SUBST ANCE POSITIVE], [SUBST ANCE\\nNEGA TIVE], [ORIGINALITY POSITIVE], [ORIGI-\\nNALITY NEGA TIVE], [SOUNDNESS POSITIVE],\\n[SOUNDNESS NEGA TIVE], [CLARITY POSI-\\nTIVE], [CLARITY NEGA TIVE], [REPLICABILITY\\nPOSITIVE], [REPLICABILIT NEGA TIVE], [MEAN-\\nINGFUL COMP ARISON POSITIVE], [MEANING-\\nFUL COMP ARISON NEGA TIVE]. Y our total output\\nshould not surpass 500 tokens.\\nZero-Shot Example Example output: [LABEL]\\nsequence. [LABEL] sequence. [LABEL] se-\\nquence......\\nFew-Shot Example Example1: [SUM-\\nMARY]This paper introduces a method to\\ndisentanglement the private and public attribute\\ninformation... Example2: [SUMMARY]The paper\\nproposes learning NN to correct for inaccuracies...\\nExample3: [SUMMARY]This paper describes a\\nmethod for segmenting 3D point clouds... Exam-\\nple4: [SUMMARY]This work introduces GQ-Net ,\\na novel technique that trains quantization friendly\\nnetworks...'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='Example3: [SUMMARY]This paper describes a\\nmethod for segmenting 3D point clouds... Exam-\\nple4: [SUMMARY]This work introduces GQ-Net ,\\na novel technique that trains quantization friendly\\nnetworks...\\nSetting 2 Given reference reviews, evaluate the\\ngenerated review quality .\\nPrompt Score the following review step by step\\nwith respect to its relevance with reference reviews\\non a continuous scale from 0 to 100. Y ou should\\ngive a relevance score, a precision score and a\\nrecall score of the review to be scored. Rele-\\nvance measures its selection of important content\\nfrom references, where relevance=0 means ’no\\nmeaning preserved’ and relevance=100 means\\n’perfect meaning’. Precision measures its correct-\\nness with respect to references, and recall mea-\\nsures its information coverage with respect to ref-\\nerences. Output format: Score for the review to be\\nscored:relevance=x, precision=x, recall=x.\\nA.3. Evaluation on RR-MCQ\\nSetting 1 Given question and paper , select use-\\nful sections.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='erences. Output format: Score for the review to be\\nscored:relevance=x, precision=x, recall=x.\\nA.3. Evaluation on RR-MCQ\\nSetting 1 Given question and paper , select use-\\nful sections.\\nPrompt Y ou are a professional reviewer in ley-\\nwords. Y ou will be given a multiple choice ques-\\ntion and the headings of a research paper in this\\nfield. Y ou need to select sections that are useful\\nto anwer the question.\\nSetting 2 Given selected sections, predict an-\\nswers.\\nPrompt Y ou are a professional reviewer in key-\\nwords. Y ou will be given some sections extracted\\nfrom a paper in this domain. Based on the given\\ncontext, you need to answer the following multiple\\nchoice question. Y ou should select one or more\\nanswer choices from A, B, C, D.\\nB. Labeling Principle\\nB.1. Review aspect\\n• [Soundness] Questions related to the sound-\\nness of claims, supporting materials and\\nmathematical results.\\n• [Clarity] Questions of requesting more expla-\\nnations.\\n• [Comparison] Questions related to the com-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='ness of claims, supporting materials and\\nmathematical results.\\n• [Clarity] Questions of requesting more expla-\\nnations.\\n• [Comparison] Questions related to the com-\\nparison with related work: whether it is precise\\nand complete.\\n• [Substance] Questions to evaluate the num-\\nber of new ideas, results and the amount of\\nwork.\\n• [Citation] Specific questions of citations with-\\nout much comparison.\\n• [Reproducibility] Questions about code avail-\\nability , settings and hyperparameters.\\n• [Novelty] Questions to evaluate the signifi-\\ncance of problem, technique, methodology , or\\ninsight.\\n• [Format] Specific questions about the paper\\nformat.\\nB.2. Content aspect\\nQuestions related to different parts of the paper .\\nThe labels are: [Empirical Result], [Method], [Re-\\nlated Work], [Dataset], [Theoretical Result], [T ask],\\n[Abstract], [Evaluation], [PDF].\\nB.3. Ability\\nThe main ability needed to solve the question. If\\nmore than one ability is required, only choose the'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='[Abstract], [Evaluation], [PDF].\\nB.3. Ability\\nThe main ability needed to solve the question. If\\nmore than one ability is required, only choose the\\nmore complex one. The following labels are or-\\ndered increasingly by their complexity .\\n• [Knowledge] Questions about domain knowl-\\nedge, not the paper .\\n• [Summarize] Questions about general de-\\nscriptions of the paper . If it requires detailed\\ninformation or analysis of the reasoning pro-\\ncess, the use the [find] label.\\n• [Compare] Questions about comparing the\\npaper to other domain knowledge.\\n• [Find] Questions about detailed information\\nand logic.\\n• [Explain] Questions about further explana-\\ntions. If the explanation needs to add content,\\nfor example extra experiments or results, then\\nuse the [add] label.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20220710)', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-24T16:10:22+08:00', 'author': 'Ruiyang Zhou ; Lu Chen ; Kai Yu', 'subject': 'main 2024', 'title': 'Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks', 'source': 'LLm_evaluation.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='9351\\n• [Add] Questions about adding content to the\\noriginal paper . For example experiments, re-\\nsults, citations, etc. If the correction of old con-\\ntent is also involved, then use the [correct] la-\\nbel.\\n• [Correct] Questions about finding errors and\\nmake modifications.\\nB.4. Extra Information\\nOnly information from referenced papers (citations\\nin the paper or in the discussion forum) are consid-\\nered extra information.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2814f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector Embedding And Vector Store\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(documents,OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ca5db37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and logic-demanding. GPT -3.5 gains espe-\n",
      "cially high correlations in predicting [Recommen-\n",
      "dation] scores under both settings (Pearson corre-\n",
      "lation 0.826 & 0.283, see detailed results for each\n",
      "aspect in T able2 column 1). However , it struggles\n",
      "in judging [Comparison] (column 4) whether the pa-\n",
      "per presents enough meaningful comparisons with\n",
      "related work, [Substance] (column 2) whether it\n",
      "contains lots of ideas and results, and [Impact] (col-\n",
      "umn 8) whether it is influential and helpful to this\n",
      "field. We may attribute the diﬀiculty to the need of\n",
      "extra scientific knowledge and details, as all three\n",
      "aspects require a rich understanding of the field.\n",
      "However , we cannot exclude the possibility of us-\n",
      "ing memorized data to successfully predict the\n",
      "[Recommendation] score (data leakage), as this\n",
      "score is the easiest to infer from other factors\n",
      "and the PeerRead dataset uses ICLR-2017 pa-\n",
      "pers. Therefore, we present a more detailed exam-\n",
      "ination of model’s ability in Section 5 on our MCQ\n"
     ]
    }
   ],
   "source": [
    "query = \"we find that GPT-4 generated reviews are strongly influenced by the given paper content\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9014493",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FAISS Vector Database\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents[:15], OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "664b1517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LREC-COLING 2024, pages 9340–9351\n",
      "20-25 May, 2024. © 2024 ELRA Language Resource Association: CC BY-NC 4.0\n",
      "9340\n",
      "Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM\n",
      "on Automatic Paper Reviewing T asks\n",
      "Ruiyang Zhou1 Lu Chen1,2, B Kai Yu1,2, B\n",
      "1 X-LANCE Lab, Department of Computer Science and Engineering\n",
      "MoE Key Lab of Artificial Intelligence, SJTU AI Institute\n",
      "Shanghai Jiao T ong University , Shanghai, China\n",
      "2 Suzhou Laboratory , Suzhou, China\n",
      "{ellenruiyang,chenlusz,kai.yu}@sjtu.edu.cn\n",
      "Abstract\n",
      "The use of large language models (LLM), especially ChatGPT , to help with research has come into practice.\n",
      "Researchers use it for timely advice and hope to obtain in-depth feedback. However , can LLM be a qualified\n",
      "and reliable reviewer? Although there already exist several review-related datasets, few works have carefully and\n",
      "thoroughly inspected model’s capability as a reviewer , especially the correctness of generated reviews. In this\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the authors of the paper Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks?\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7f01bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## With lancedb\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "db = LanceDB.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54efd8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LREC-COLING 2024, pages 9340–9351\n",
      "20-25 May, 2024. © 2024 ELRA Language Resource Association: CC BY-NC 4.0\n",
      "9340\n",
      "Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM\n",
      "on Automatic Paper Reviewing T asks\n",
      "Ruiyang Zhou1 Lu Chen1,2, B Kai Yu1,2, B\n",
      "1 X-LANCE Lab, Department of Computer Science and Engineering\n",
      "MoE Key Lab of Artificial Intelligence, SJTU AI Institute\n",
      "Shanghai Jiao T ong University , Shanghai, China\n",
      "2 Suzhou Laboratory , Suzhou, China\n",
      "{ellenruiyang,chenlusz,kai.yu}@sjtu.edu.cn\n",
      "Abstract\n",
      "The use of large language models (LLM), especially ChatGPT , to help with research has come into practice.\n",
      "Researchers use it for timely advice and hope to obtain in-depth feedback. However , can LLM be a qualified\n",
      "and reliable reviewer? Although there already exist several review-related datasets, few works have carefully and\n",
      "thoroughly inspected model’s capability as a reviewer , especially the correctness of generated reviews. In this\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the authors of the paper Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks?\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b33d4",
   "metadata": {},
   "source": [
    "Advance RAG_Pipeline with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2e9e6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\i39106\\AppData\\Local\\Temp\\ipykernel_6920\\1927076850.py:5: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  db=FAISS.from_documents(documents[:30],OpenAIEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "#from langchain_community.embeddings import OllamaEmbeddings  ## for open source models\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db=FAISS.from_documents(documents[:30],OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c31c1390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x29c9a745600>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cf2b32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LREC-COLING 2024, pages 9340–9351\\n20-25 May, 2024. © 2024 ELRA Language Resource Association: CC BY-NC 4.0\\n9340\\nIs LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM\\non Automatic Paper Reviewing T asks\\nRuiyang Zhou1 Lu Chen1,2, B Kai Yu1,2, B\\n1 X-LANCE Lab, Department of Computer Science and Engineering\\nMoE Key Lab of Artificial Intelligence, SJTU AI Institute\\nShanghai Jiao T ong University , Shanghai, China\\n2 Suzhou Laboratory , Suzhou, China\\n{ellenruiyang,chenlusz,kai.yu}@sjtu.edu.cn\\nAbstract\\nThe use of large language models (LLM), especially ChatGPT , to help with research has come into practice.\\nResearchers use it for timely advice and hope to obtain in-depth feedback. However , can LLM be a qualified\\nand reliable reviewer? Although there already exist several review-related datasets, few works have carefully and\\nthoroughly inspected model’s capability as a reviewer , especially the correctness of generated reviews. In this'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"Can LLM be a qualified and reliable automatic reviewer?\"\n",
    "result=db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.llms import Ollama\n",
    "## Load Ollama LAMA2 LLM model\n",
    "#llm=Ollama(model=\"llama2\")\n",
    "#llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ea02c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000029C9A746E30>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000029C9A9E1A50>, root_client=<openai.OpenAI object at 0x0000029C9A6BBD90>, root_async_client=<openai.AsyncOpenAI object at 0x0000029C9A746DA0>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1eeebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    " \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e0e641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain Introduction\n",
    "## Create Stuff Docment Chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2398d66",
   "metadata": {},
   "source": [
    "\n",
    "Retrievers: A retriever is an interface that returns documents given\n",
    " an unstructured query. It is more general than a vector store.\n",
    " A retriever does not need to be able to store documents, only to \n",
    " return (or retrieve) them. Vector stores can be used as the backbone\n",
    " of a retriever, but there are other types of retrievers as well. \n",
    " https://python.langchain.com/docs/modules/data_connection/retrievers/   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01fb9f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000029C9A745600>, search_kwargs={})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "retriever=db.as_retriever()\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65316119",
   "metadata": {},
   "source": [
    "Retrieval chain:This chain takes in a user inquiry, which is then\n",
    "passed to the retriever to fetch relevant documents. Those documents \n",
    "(and original inputs) are then passed to an LLM to generate a response\n",
    "https://python.langchain.com/docs/modules/chains/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bb373b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c79f938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retrieval_chain.invoke({\"input\":\"what is Automatic review generation\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1943ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Automatic review generation is the task of using models to automatically generate reviews of scientific papers, typically in the domain of computer science. Datasets such as PeerRead, ASAP, ReviewRobot, MOPRD, and NLPEER contain scientific papers and their corresponding peer reviews. However, due to the difficulty of directly generating review texts, various types of annotations have been proposed to evaluate the generated reviews. The most common labels include sentiment polarity, review aspect, and the text aspect being commented on. The goal of automatic review generation is to assess the ability of a model to generate reviews that are accurate and reliable.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff69013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
